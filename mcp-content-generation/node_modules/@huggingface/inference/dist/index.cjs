"use strict";
var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __export = (target, all) => {
  for (var name2 in all)
    __defProp(target, name2, { get: all[name2], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);

// src/index.ts
var src_exports = {};
__export(src_exports, {
  HfInference: () => HfInference,
  INFERENCE_PROVIDERS: () => INFERENCE_PROVIDERS,
  InferenceClient: () => InferenceClient,
  InferenceClientEndpoint: () => InferenceClientEndpoint,
  InferenceOutputError: () => InferenceOutputError,
  audioClassification: () => audioClassification,
  audioToAudio: () => audioToAudio,
  automaticSpeechRecognition: () => automaticSpeechRecognition,
  chatCompletion: () => chatCompletion,
  chatCompletionStream: () => chatCompletionStream,
  documentQuestionAnswering: () => documentQuestionAnswering,
  featureExtraction: () => featureExtraction,
  fillMask: () => fillMask,
  imageClassification: () => imageClassification,
  imageSegmentation: () => imageSegmentation,
  imageToImage: () => imageToImage,
  imageToText: () => imageToText,
  objectDetection: () => objectDetection,
  questionAnswering: () => questionAnswering,
  request: () => request,
  sentenceSimilarity: () => sentenceSimilarity,
  snippets: () => snippets_exports,
  streamingRequest: () => streamingRequest,
  summarization: () => summarization,
  tableQuestionAnswering: () => tableQuestionAnswering,
  tabularClassification: () => tabularClassification,
  tabularRegression: () => tabularRegression,
  textClassification: () => textClassification,
  textGeneration: () => textGeneration,
  textGenerationStream: () => textGenerationStream,
  textToImage: () => textToImage,
  textToSpeech: () => textToSpeech,
  textToVideo: () => textToVideo,
  tokenClassification: () => tokenClassification,
  translation: () => translation,
  visualQuestionAnswering: () => visualQuestionAnswering,
  zeroShotClassification: () => zeroShotClassification,
  zeroShotImageClassification: () => zeroShotImageClassification
});
module.exports = __toCommonJS(src_exports);

// src/tasks/index.ts
var tasks_exports = {};
__export(tasks_exports, {
  audioClassification: () => audioClassification,
  audioToAudio: () => audioToAudio,
  automaticSpeechRecognition: () => automaticSpeechRecognition,
  chatCompletion: () => chatCompletion,
  chatCompletionStream: () => chatCompletionStream,
  documentQuestionAnswering: () => documentQuestionAnswering,
  featureExtraction: () => featureExtraction,
  fillMask: () => fillMask,
  imageClassification: () => imageClassification,
  imageSegmentation: () => imageSegmentation,
  imageToImage: () => imageToImage,
  imageToText: () => imageToText,
  objectDetection: () => objectDetection,
  questionAnswering: () => questionAnswering,
  request: () => request,
  sentenceSimilarity: () => sentenceSimilarity,
  streamingRequest: () => streamingRequest,
  summarization: () => summarization,
  tableQuestionAnswering: () => tableQuestionAnswering,
  tabularClassification: () => tabularClassification,
  tabularRegression: () => tabularRegression,
  textClassification: () => textClassification,
  textGeneration: () => textGeneration,
  textGenerationStream: () => textGenerationStream,
  textToImage: () => textToImage,
  textToSpeech: () => textToSpeech,
  textToVideo: () => textToVideo,
  tokenClassification: () => tokenClassification,
  translation: () => translation,
  visualQuestionAnswering: () => visualQuestionAnswering,
  zeroShotClassification: () => zeroShotClassification,
  zeroShotImageClassification: () => zeroShotImageClassification
});

// src/lib/InferenceOutputError.ts
var InferenceOutputError = class extends TypeError {
  constructor(message) {
    super(
      `Invalid inference output: ${message}. Use the 'request' method with the same parameters to do a custom call with no type checking.`
    );
    this.name = "InferenceOutputError";
  }
};

// src/utils/delay.ts
function delay(ms) {
  return new Promise((resolve) => {
    setTimeout(() => resolve(), ms);
  });
}

// src/utils/pick.ts
function pick(o, props) {
  return Object.assign(
    {},
    ...props.map((prop) => {
      if (o[prop] !== void 0) {
        return { [prop]: o[prop] };
      }
    })
  );
}

// src/utils/typedInclude.ts
function typedInclude(arr, v) {
  return arr.includes(v);
}

// src/utils/omit.ts
function omit(o, props) {
  const propsArr = Array.isArray(props) ? props : [props];
  const letsKeep = Object.keys(o).filter((prop) => !typedInclude(propsArr, prop));
  return pick(o, letsKeep);
}

// src/config.ts
var HF_HUB_URL = "https://huggingface.co";
var HF_ROUTER_URL = "https://router.huggingface.co";
var HF_HEADER_X_BILL_TO = "X-HF-Bill-To";

// src/utils/toArray.ts
function toArray(obj) {
  if (Array.isArray(obj)) {
    return obj;
  }
  return [obj];
}

// src/providers/providerHelper.ts
var TaskProviderHelper = class {
  constructor(provider, baseUrl, clientSideRoutingOnly = false) {
    this.provider = provider;
    this.baseUrl = baseUrl;
    this.clientSideRoutingOnly = clientSideRoutingOnly;
  }
  /**
   * Prepare the base URL for the request
   */
  makeBaseUrl(params) {
    return params.authMethod !== "provider-key" ? `${HF_ROUTER_URL}/${this.provider}` : this.baseUrl;
  }
  /**
   * Prepare the body for the request
   */
  makeBody(params) {
    if ("data" in params.args && !!params.args.data) {
      return params.args.data;
    }
    return JSON.stringify(this.preparePayload(params));
  }
  /**
   * Prepare the URL for the request
   */
  makeUrl(params) {
    const baseUrl = this.makeBaseUrl(params);
    const route = this.makeRoute(params).replace(/^\/+/, "");
    return `${baseUrl}/${route}`;
  }
  /**
   * Prepare the headers for the request
   */
  prepareHeaders(params, isBinary) {
    const headers = { Authorization: `Bearer ${params.accessToken}` };
    if (!isBinary) {
      headers["Content-Type"] = "application/json";
    }
    return headers;
  }
};
var BaseConversationalTask = class extends TaskProviderHelper {
  constructor(provider, baseUrl, clientSideRoutingOnly = false) {
    super(provider, baseUrl, clientSideRoutingOnly);
  }
  makeRoute() {
    return "v1/chat/completions";
  }
  preparePayload(params) {
    return {
      ...params.args,
      model: params.model
    };
  }
  async getResponse(response) {
    if (typeof response === "object" && Array.isArray(response?.choices) && typeof response?.created === "number" && typeof response?.id === "string" && typeof response?.model === "string" && /// Together.ai and Nebius do not output a system_fingerprint
    (response.system_fingerprint === void 0 || response.system_fingerprint === null || typeof response.system_fingerprint === "string") && typeof response?.usage === "object") {
      return response;
    }
    throw new InferenceOutputError("Expected ChatCompletionOutput");
  }
};
var BaseTextGenerationTask = class extends TaskProviderHelper {
  constructor(provider, baseUrl, clientSideRoutingOnly = false) {
    super(provider, baseUrl, clientSideRoutingOnly);
  }
  preparePayload(params) {
    return {
      ...params.args,
      model: params.model
    };
  }
  makeRoute() {
    return "v1/completions";
  }
  async getResponse(response) {
    const res = toArray(response);
    if (Array.isArray(res) && res.length > 0 && res.every(
      (x) => typeof x === "object" && !!x && "generated_text" in x && typeof x.generated_text === "string"
    )) {
      return res[0];
    }
    throw new InferenceOutputError("Expected Array<{generated_text: string}>");
  }
};

// src/providers/black-forest-labs.ts
var BLACK_FOREST_LABS_AI_API_BASE_URL = "https://api.us1.bfl.ai";
var BlackForestLabsTextToImageTask = class extends TaskProviderHelper {
  constructor() {
    super("black-forest-labs", BLACK_FOREST_LABS_AI_API_BASE_URL);
  }
  preparePayload(params) {
    return {
      ...omit(params.args, ["inputs", "parameters"]),
      ...params.args.parameters,
      prompt: params.args.inputs
    };
  }
  prepareHeaders(params, binary) {
    const headers = {
      Authorization: params.authMethod !== "provider-key" ? `Bearer ${params.accessToken}` : `X-Key ${params.accessToken}`
    };
    if (!binary) {
      headers["Content-Type"] = "application/json";
    }
    return headers;
  }
  makeRoute(params) {
    if (!params) {
      throw new Error("Params are required");
    }
    return `/v1/${params.model}`;
  }
  async getResponse(response, url, headers, outputType) {
    const urlObj = new URL(response.polling_url);
    for (let step = 0; step < 5; step++) {
      await delay(1e3);
      console.debug(`Polling Black Forest Labs API for the result... ${step + 1}/5`);
      urlObj.searchParams.set("attempt", step.toString(10));
      const resp = await fetch(urlObj, { headers: { "Content-Type": "application/json" } });
      if (!resp.ok) {
        throw new InferenceOutputError("Failed to fetch result from black forest labs API");
      }
      const payload = await resp.json();
      if (typeof payload === "object" && payload && "status" in payload && typeof payload.status === "string" && payload.status === "Ready" && "result" in payload && typeof payload.result === "object" && payload.result && "sample" in payload.result && typeof payload.result.sample === "string") {
        if (outputType === "url") {
          return payload.result.sample;
        }
        const image = await fetch(payload.result.sample);
        return await image.blob();
      }
    }
    throw new InferenceOutputError("Failed to fetch result from black forest labs API");
  }
};

// src/providers/cerebras.ts
var CerebrasConversationalTask = class extends BaseConversationalTask {
  constructor() {
    super("cerebras", "https://api.cerebras.ai");
  }
};

// src/providers/cohere.ts
var CohereConversationalTask = class extends BaseConversationalTask {
  constructor() {
    super("cohere", "https://api.cohere.com");
  }
  makeRoute() {
    return "/compatibility/v1/chat/completions";
  }
};

// src/lib/isUrl.ts
function isUrl(modelOrUrl) {
  return /^http(s?):/.test(modelOrUrl) || modelOrUrl.startsWith("/");
}

// src/providers/fal-ai.ts
var FAL_AI_SUPPORTED_BLOB_TYPES = ["audio/mpeg", "audio/mp4", "audio/wav", "audio/x-wav"];
var FalAITask = class extends TaskProviderHelper {
  constructor(url) {
    super("fal-ai", url || "https://fal.run");
  }
  preparePayload(params) {
    return params.args;
  }
  makeRoute(params) {
    return `/${params.model}`;
  }
  prepareHeaders(params, binary) {
    const headers = {
      Authorization: params.authMethod !== "provider-key" ? `Bearer ${params.accessToken}` : `Key ${params.accessToken}`
    };
    if (!binary) {
      headers["Content-Type"] = "application/json";
    }
    return headers;
  }
};
function buildLoraPath(modelId, adapterWeightsPath) {
  return `${HF_HUB_URL}/${modelId}/resolve/main/${adapterWeightsPath}`;
}
var FalAITextToImageTask = class extends FalAITask {
  preparePayload(params) {
    const payload = {
      ...omit(params.args, ["inputs", "parameters"]),
      ...params.args.parameters,
      sync_mode: true,
      prompt: params.args.inputs,
      ...params.mapping?.adapter === "lora" && params.mapping.adapterWeightsPath ? {
        loras: [
          {
            path: buildLoraPath(params.mapping.hfModelId, params.mapping.adapterWeightsPath),
            scale: 1
          }
        ]
      } : void 0
    };
    if (params.mapping?.adapter === "lora" && params.mapping.adapterWeightsPath) {
      payload.loras = [
        {
          path: buildLoraPath(params.mapping.hfModelId, params.mapping.adapterWeightsPath),
          scale: 1
        }
      ];
      if (params.mapping.providerId === "fal-ai/lora") {
        payload.model_name = "stabilityai/stable-diffusion-xl-base-1.0";
      }
    }
    return payload;
  }
  async getResponse(response, outputType) {
    if (typeof response === "object" && "images" in response && Array.isArray(response.images) && response.images.length > 0 && "url" in response.images[0] && typeof response.images[0].url === "string") {
      if (outputType === "url") {
        return response.images[0].url;
      }
      const urlResponse = await fetch(response.images[0].url);
      return await urlResponse.blob();
    }
    throw new InferenceOutputError("Expected Fal.ai text-to-image response format");
  }
};
var FalAITextToVideoTask = class extends FalAITask {
  constructor() {
    super("https://queue.fal.run");
  }
  makeRoute(params) {
    if (params.authMethod !== "provider-key") {
      return `/${params.model}?_subdomain=queue`;
    }
    return `/${params.model}`;
  }
  preparePayload(params) {
    return {
      ...omit(params.args, ["inputs", "parameters"]),
      ...params.args.parameters,
      prompt: params.args.inputs
    };
  }
  async getResponse(response, url, headers) {
    if (!url || !headers) {
      throw new InferenceOutputError("URL and headers are required for text-to-video task");
    }
    const requestId = response.request_id;
    if (!requestId) {
      throw new InferenceOutputError("No request ID found in the response");
    }
    let status = response.status;
    const parsedUrl = new URL(url);
    const baseUrl = `${parsedUrl.protocol}//${parsedUrl.host}${parsedUrl.host === "router.huggingface.co" ? "/fal-ai" : ""}`;
    const modelId = new URL(response.response_url).pathname;
    const queryParams = parsedUrl.search;
    const statusUrl = `${baseUrl}${modelId}/status${queryParams}`;
    const resultUrl = `${baseUrl}${modelId}${queryParams}`;
    while (status !== "COMPLETED") {
      await delay(500);
      const statusResponse = await fetch(statusUrl, { headers });
      if (!statusResponse.ok) {
        throw new InferenceOutputError("Failed to fetch response status from fal-ai API");
      }
      try {
        status = (await statusResponse.json()).status;
      } catch (error) {
        throw new InferenceOutputError("Failed to parse status response from fal-ai API");
      }
    }
    const resultResponse = await fetch(resultUrl, { headers });
    let result;
    try {
      result = await resultResponse.json();
    } catch (error) {
      throw new InferenceOutputError("Failed to parse result response from fal-ai API");
    }
    if (typeof result === "object" && !!result && "video" in result && typeof result.video === "object" && !!result.video && "url" in result.video && typeof result.video.url === "string" && isUrl(result.video.url)) {
      const urlResponse = await fetch(result.video.url);
      return await urlResponse.blob();
    } else {
      throw new InferenceOutputError(
        "Expected { video: { url: string } } result format, got instead: " + JSON.stringify(result)
      );
    }
  }
};
var FalAIAutomaticSpeechRecognitionTask = class extends FalAITask {
  prepareHeaders(params, binary) {
    const headers = super.prepareHeaders(params, binary);
    headers["Content-Type"] = "application/json";
    return headers;
  }
  async getResponse(response) {
    const res = response;
    if (typeof res?.text !== "string") {
      throw new InferenceOutputError(
        `Expected { text: string } format from Fal.ai Automatic Speech Recognition, got: ${JSON.stringify(response)}`
      );
    }
    return { text: res.text };
  }
};
var FalAITextToSpeechTask = class extends FalAITask {
  preparePayload(params) {
    return {
      ...omit(params.args, ["inputs", "parameters"]),
      ...params.args.parameters,
      lyrics: params.args.inputs
    };
  }
  async getResponse(response) {
    const res = response;
    if (typeof res?.audio?.url !== "string") {
      throw new InferenceOutputError(
        `Expected { audio: { url: string } } format from Fal.ai Text-to-Speech, got: ${JSON.stringify(response)}`
      );
    }
    try {
      const urlResponse = await fetch(res.audio.url);
      if (!urlResponse.ok) {
        throw new Error(`Failed to fetch audio from ${res.audio.url}: ${urlResponse.statusText}`);
      }
      return await urlResponse.blob();
    } catch (error) {
      throw new InferenceOutputError(
        `Error fetching or processing audio from Fal.ai Text-to-Speech URL: ${res.audio.url}. ${error instanceof Error ? error.message : String(error)}`
      );
    }
  }
};

// src/providers/fireworks-ai.ts
var FireworksConversationalTask = class extends BaseConversationalTask {
  constructor() {
    super("fireworks-ai", "https://api.fireworks.ai");
  }
  makeRoute() {
    return "/inference/v1/chat/completions";
  }
};

// src/providers/hf-inference.ts
var EQUIVALENT_SENTENCE_TRANSFORMERS_TASKS = ["feature-extraction", "sentence-similarity"];
var HFInferenceTask = class extends TaskProviderHelper {
  constructor() {
    super("hf-inference", `${HF_ROUTER_URL}/hf-inference`);
  }
  preparePayload(params) {
    return params.args;
  }
  makeUrl(params) {
    if (params.model.startsWith("http://") || params.model.startsWith("https://")) {
      return params.model;
    }
    return super.makeUrl(params);
  }
  makeRoute(params) {
    if (params.task && ["feature-extraction", "sentence-similarity"].includes(params.task)) {
      return `pipeline/${params.task}/${params.model}`;
    }
    return `models/${params.model}`;
  }
  async getResponse(response) {
    return response;
  }
};
var HFInferenceTextToImageTask = class extends HFInferenceTask {
  async getResponse(response, url, headers, outputType) {
    if (!response) {
      throw new InferenceOutputError("response is undefined");
    }
    if (typeof response == "object") {
      if ("data" in response && Array.isArray(response.data) && response.data[0].b64_json) {
        const base64Data = response.data[0].b64_json;
        if (outputType === "url") {
          return `data:image/jpeg;base64,${base64Data}`;
        }
        const base64Response = await fetch(`data:image/jpeg;base64,${base64Data}`);
        return await base64Response.blob();
      }
      if ("output" in response && Array.isArray(response.output)) {
        if (outputType === "url") {
          return response.output[0];
        }
        const urlResponse = await fetch(response.output[0]);
        const blob = await urlResponse.blob();
        return blob;
      }
    }
    if (response instanceof Blob) {
      if (outputType === "url") {
        const b64 = await response.arrayBuffer().then((buf) => Buffer.from(buf).toString("base64"));
        return `data:image/jpeg;base64,${b64}`;
      }
      return response;
    }
    throw new InferenceOutputError("Expected a Blob ");
  }
};
var HFInferenceConversationalTask = class extends HFInferenceTask {
  makeUrl(params) {
    let url;
    if (params.model.startsWith("http://") || params.model.startsWith("https://")) {
      url = params.model.trim();
    } else {
      url = `${this.makeBaseUrl(params)}/models/${params.model}`;
    }
    url = url.replace(/\/+$/, "");
    if (url.endsWith("/v1")) {
      url += "/chat/completions";
    } else if (!url.endsWith("/chat/completions")) {
      url += "/v1/chat/completions";
    }
    return url;
  }
  preparePayload(params) {
    return {
      ...params.args,
      model: params.model
    };
  }
  async getResponse(response) {
    return response;
  }
};
var HFInferenceTextGenerationTask = class extends HFInferenceTask {
  async getResponse(response) {
    const res = toArray(response);
    if (Array.isArray(res) && res.every((x) => "generated_text" in x && typeof x?.generated_text === "string")) {
      return res?.[0];
    }
    throw new InferenceOutputError("Expected Array<{generated_text: string}>");
  }
};
var HFInferenceAudioClassificationTask = class extends HFInferenceTask {
  async getResponse(response) {
    if (Array.isArray(response) && response.every(
      (x) => typeof x === "object" && x !== null && typeof x.label === "string" && typeof x.score === "number"
    )) {
      return response;
    }
    throw new InferenceOutputError("Expected Array<{label: string, score: number}> but received different format");
  }
};
var HFInferenceAutomaticSpeechRecognitionTask = class extends HFInferenceTask {
  async getResponse(response) {
    return response;
  }
};
var HFInferenceAudioToAudioTask = class extends HFInferenceTask {
  async getResponse(response) {
    if (!Array.isArray(response)) {
      throw new InferenceOutputError("Expected Array");
    }
    if (!response.every((elem) => {
      return typeof elem === "object" && elem && "label" in elem && typeof elem.label === "string" && "content-type" in elem && typeof elem["content-type"] === "string" && "blob" in elem && typeof elem.blob === "string";
    })) {
      throw new InferenceOutputError("Expected Array<{label: string, audio: Blob}>");
    }
    return response;
  }
};
var HFInferenceDocumentQuestionAnsweringTask = class extends HFInferenceTask {
  async getResponse(response) {
    if (Array.isArray(response) && response.every(
      (elem) => typeof elem === "object" && !!elem && typeof elem?.answer === "string" && (typeof elem.end === "number" || typeof elem.end === "undefined") && (typeof elem.score === "number" || typeof elem.score === "undefined") && (typeof elem.start === "number" || typeof elem.start === "undefined")
    )) {
      return response[0];
    }
    throw new InferenceOutputError("Expected Array<{answer: string, end: number, score: number, start: number}>");
  }
};
var HFInferenceFeatureExtractionTask = class extends HFInferenceTask {
  async getResponse(response) {
    const isNumArrayRec = (arr, maxDepth, curDepth = 0) => {
      if (curDepth > maxDepth)
        return false;
      if (arr.every((x) => Array.isArray(x))) {
        return arr.every((x) => isNumArrayRec(x, maxDepth, curDepth + 1));
      } else {
        return arr.every((x) => typeof x === "number");
      }
    };
    if (Array.isArray(response) && isNumArrayRec(response, 3, 0)) {
      return response;
    }
    throw new InferenceOutputError("Expected Array<number[][][] | number[][] | number[] | number>");
  }
};
var HFInferenceImageClassificationTask = class extends HFInferenceTask {
  async getResponse(response) {
    if (Array.isArray(response) && response.every((x) => typeof x.label === "string" && typeof x.score === "number")) {
      return response;
    }
    throw new InferenceOutputError("Expected Array<{label: string, score: number}>");
  }
};
var HFInferenceImageSegmentationTask = class extends HFInferenceTask {
  async getResponse(response) {
    if (Array.isArray(response) && response.every((x) => typeof x.label === "string" && typeof x.mask === "string" && typeof x.score === "number")) {
      return response;
    }
    throw new InferenceOutputError("Expected Array<{label: string, mask: string, score: number}>");
  }
};
var HFInferenceImageToTextTask = class extends HFInferenceTask {
  async getResponse(response) {
    if (typeof response?.generated_text !== "string") {
      throw new InferenceOutputError("Expected {generated_text: string}");
    }
    return response;
  }
};
var HFInferenceImageToImageTask = class extends HFInferenceTask {
  async getResponse(response) {
    if (response instanceof Blob) {
      return response;
    }
    throw new InferenceOutputError("Expected Blob");
  }
};
var HFInferenceObjectDetectionTask = class extends HFInferenceTask {
  async getResponse(response) {
    if (Array.isArray(response) && response.every(
      (x) => typeof x.label === "string" && typeof x.score === "number" && typeof x.box.xmin === "number" && typeof x.box.ymin === "number" && typeof x.box.xmax === "number" && typeof x.box.ymax === "number"
    )) {
      return response;
    }
    throw new InferenceOutputError(
      "Expected Array<{label: string, score: number, box: {xmin: number, ymin: number, xmax: number, ymax: number}}>"
    );
  }
};
var HFInferenceZeroShotImageClassificationTask = class extends HFInferenceTask {
  async getResponse(response) {
    if (Array.isArray(response) && response.every((x) => typeof x.label === "string" && typeof x.score === "number")) {
      return response;
    }
    throw new InferenceOutputError("Expected Array<{label: string, score: number}>");
  }
};
var HFInferenceTextClassificationTask = class extends HFInferenceTask {
  async getResponse(response) {
    const output = response?.[0];
    if (Array.isArray(output) && output.every((x) => typeof x?.label === "string" && typeof x.score === "number")) {
      return output;
    }
    throw new InferenceOutputError("Expected Array<{label: string, score: number}>");
  }
};
var HFInferenceQuestionAnsweringTask = class extends HFInferenceTask {
  async getResponse(response) {
    if (Array.isArray(response) ? response.every(
      (elem) => typeof elem === "object" && !!elem && typeof elem.answer === "string" && typeof elem.end === "number" && typeof elem.score === "number" && typeof elem.start === "number"
    ) : typeof response === "object" && !!response && typeof response.answer === "string" && typeof response.end === "number" && typeof response.score === "number" && typeof response.start === "number") {
      return Array.isArray(response) ? response[0] : response;
    }
    throw new InferenceOutputError("Expected Array<{answer: string, end: number, score: number, start: number}>");
  }
};
var HFInferenceFillMaskTask = class extends HFInferenceTask {
  async getResponse(response) {
    if (Array.isArray(response) && response.every(
      (x) => typeof x.score === "number" && typeof x.sequence === "string" && typeof x.token === "number" && typeof x.token_str === "string"
    )) {
      return response;
    }
    throw new InferenceOutputError(
      "Expected Array<{score: number, sequence: string, token: number, token_str: string}>"
    );
  }
};
var HFInferenceZeroShotClassificationTask = class extends HFInferenceTask {
  async getResponse(response) {
    if (Array.isArray(response) && response.every(
      (x) => Array.isArray(x.labels) && x.labels.every((_label) => typeof _label === "string") && Array.isArray(x.scores) && x.scores.every((_score) => typeof _score === "number") && typeof x.sequence === "string"
    )) {
      return response;
    }
    throw new InferenceOutputError("Expected Array<{labels: string[], scores: number[], sequence: string}>");
  }
};
var HFInferenceSentenceSimilarityTask = class extends HFInferenceTask {
  async getResponse(response) {
    if (Array.isArray(response) && response.every((x) => typeof x === "number")) {
      return response;
    }
    throw new InferenceOutputError("Expected Array<number>");
  }
};
var HFInferenceTableQuestionAnsweringTask = class extends HFInferenceTask {
  static validate(elem) {
    return typeof elem === "object" && !!elem && "aggregator" in elem && typeof elem.aggregator === "string" && "answer" in elem && typeof elem.answer === "string" && "cells" in elem && Array.isArray(elem.cells) && elem.cells.every((x) => typeof x === "string") && "coordinates" in elem && Array.isArray(elem.coordinates) && elem.coordinates.every(
      (coord) => Array.isArray(coord) && coord.every((x) => typeof x === "number")
    );
  }
  async getResponse(response) {
    if (Array.isArray(response) && Array.isArray(response) ? response.every((elem) => HFInferenceTableQuestionAnsweringTask.validate(elem)) : HFInferenceTableQuestionAnsweringTask.validate(response)) {
      return Array.isArray(response) ? response[0] : response;
    }
    throw new InferenceOutputError(
      "Expected {aggregator: string, answer: string, cells: string[], coordinates: number[][]}"
    );
  }
};
var HFInferenceTokenClassificationTask = class extends HFInferenceTask {
  async getResponse(response) {
    if (Array.isArray(response) && response.every(
      (x) => typeof x.end === "number" && typeof x.entity_group === "string" && typeof x.score === "number" && typeof x.start === "number" && typeof x.word === "string"
    )) {
      return response;
    }
    throw new InferenceOutputError(
      "Expected Array<{end: number, entity_group: string, score: number, start: number, word: string}>"
    );
  }
};
var HFInferenceTranslationTask = class extends HFInferenceTask {
  async getResponse(response) {
    if (Array.isArray(response) && response.every((x) => typeof x?.translation_text === "string")) {
      return response?.length === 1 ? response?.[0] : response;
    }
    throw new InferenceOutputError("Expected Array<{translation_text: string}>");
  }
};
var HFInferenceSummarizationTask = class extends HFInferenceTask {
  async getResponse(response) {
    if (Array.isArray(response) && response.every((x) => typeof x?.summary_text === "string")) {
      return response?.[0];
    }
    throw new InferenceOutputError("Expected Array<{summary_text: string}>");
  }
};
var HFInferenceTextToSpeechTask = class extends HFInferenceTask {
  async getResponse(response) {
    return response;
  }
};
var HFInferenceTabularClassificationTask = class extends HFInferenceTask {
  async getResponse(response) {
    if (Array.isArray(response) && response.every((x) => typeof x === "number")) {
      return response;
    }
    throw new InferenceOutputError("Expected Array<number>");
  }
};
var HFInferenceVisualQuestionAnsweringTask = class extends HFInferenceTask {
  async getResponse(response) {
    if (Array.isArray(response) && response.every(
      (elem) => typeof elem === "object" && !!elem && typeof elem?.answer === "string" && typeof elem.score === "number"
    )) {
      return response[0];
    }
    throw new InferenceOutputError("Expected Array<{answer: string, score: number}>");
  }
};
var HFInferenceTabularRegressionTask = class extends HFInferenceTask {
  async getResponse(response) {
    if (Array.isArray(response) && response.every((x) => typeof x === "number")) {
      return response;
    }
    throw new InferenceOutputError("Expected Array<number>");
  }
};
var HFInferenceTextToAudioTask = class extends HFInferenceTask {
  async getResponse(response) {
    return response;
  }
};

// src/providers/hyperbolic.ts
var HYPERBOLIC_API_BASE_URL = "https://api.hyperbolic.xyz";
var HyperbolicConversationalTask = class extends BaseConversationalTask {
  constructor() {
    super("hyperbolic", HYPERBOLIC_API_BASE_URL);
  }
};
var HyperbolicTextGenerationTask = class extends BaseTextGenerationTask {
  constructor() {
    super("hyperbolic", HYPERBOLIC_API_BASE_URL);
  }
  makeRoute() {
    return "v1/chat/completions";
  }
  preparePayload(params) {
    return {
      messages: [{ content: params.args.inputs, role: "user" }],
      ...params.args.parameters ? {
        max_tokens: params.args.parameters.max_new_tokens,
        ...omit(params.args.parameters, "max_new_tokens")
      } : void 0,
      ...omit(params.args, ["inputs", "parameters"]),
      model: params.model
    };
  }
  async getResponse(response) {
    if (typeof response === "object" && "choices" in response && Array.isArray(response?.choices) && typeof response?.model === "string") {
      const completion = response.choices[0];
      return {
        generated_text: completion.message.content
      };
    }
    throw new InferenceOutputError("Expected Hyperbolic text generation response format");
  }
};
var HyperbolicTextToImageTask = class extends TaskProviderHelper {
  constructor() {
    super("hyperbolic", HYPERBOLIC_API_BASE_URL);
  }
  makeRoute(params) {
    return `/v1/images/generations`;
  }
  preparePayload(params) {
    return {
      ...omit(params.args, ["inputs", "parameters"]),
      ...params.args.parameters,
      prompt: params.args.inputs,
      model_name: params.model
    };
  }
  async getResponse(response, url, headers, outputType) {
    if (typeof response === "object" && "images" in response && Array.isArray(response.images) && response.images[0] && typeof response.images[0].image === "string") {
      if (outputType === "url") {
        return `data:image/jpeg;base64,${response.images[0].image}`;
      }
      return fetch(`data:image/jpeg;base64,${response.images[0].image}`).then((res) => res.blob());
    }
    throw new InferenceOutputError("Expected Hyperbolic text-to-image response format");
  }
};

// src/providers/nebius.ts
var NEBIUS_API_BASE_URL = "https://api.studio.nebius.ai";
var NebiusConversationalTask = class extends BaseConversationalTask {
  constructor() {
    super("nebius", NEBIUS_API_BASE_URL);
  }
};
var NebiusTextGenerationTask = class extends BaseTextGenerationTask {
  constructor() {
    super("nebius", NEBIUS_API_BASE_URL);
  }
};
var NebiusTextToImageTask = class extends TaskProviderHelper {
  constructor() {
    super("nebius", NEBIUS_API_BASE_URL);
  }
  preparePayload(params) {
    return {
      ...omit(params.args, ["inputs", "parameters"]),
      ...params.args.parameters,
      response_format: "b64_json",
      prompt: params.args.inputs,
      model: params.model
    };
  }
  makeRoute(params) {
    return "v1/images/generations";
  }
  async getResponse(response, url, headers, outputType) {
    if (typeof response === "object" && "data" in response && Array.isArray(response.data) && response.data.length > 0 && "b64_json" in response.data[0] && typeof response.data[0].b64_json === "string") {
      const base64Data = response.data[0].b64_json;
      if (outputType === "url") {
        return `data:image/jpeg;base64,${base64Data}`;
      }
      return fetch(`data:image/jpeg;base64,${base64Data}`).then((res) => res.blob());
    }
    throw new InferenceOutputError("Expected Nebius text-to-image response format");
  }
};

// src/providers/novita.ts
var NOVITA_API_BASE_URL = "https://api.novita.ai";
var NovitaTextGenerationTask = class extends BaseTextGenerationTask {
  constructor() {
    super("novita", NOVITA_API_BASE_URL);
  }
  makeRoute() {
    return "/v3/openai/chat/completions";
  }
};
var NovitaConversationalTask = class extends BaseConversationalTask {
  constructor() {
    super("novita", NOVITA_API_BASE_URL);
  }
  makeRoute() {
    return "/v3/openai/chat/completions";
  }
};

// src/providers/openai.ts
var OPENAI_API_BASE_URL = "https://api.openai.com";
var OpenAIConversationalTask = class extends BaseConversationalTask {
  constructor() {
    super("openai", OPENAI_API_BASE_URL, true);
  }
};

// src/providers/replicate.ts
var ReplicateTask = class extends TaskProviderHelper {
  constructor(url) {
    super("replicate", url || "https://api.replicate.com");
  }
  makeRoute(params) {
    if (params.model.includes(":")) {
      return "v1/predictions";
    }
    return `v1/models/${params.model}/predictions`;
  }
  preparePayload(params) {
    return {
      input: {
        ...omit(params.args, ["inputs", "parameters"]),
        ...params.args.parameters,
        prompt: params.args.inputs
      },
      version: params.model.includes(":") ? params.model.split(":")[1] : void 0
    };
  }
  prepareHeaders(params, binary) {
    const headers = { Authorization: `Bearer ${params.accessToken}`, Prefer: "wait" };
    if (!binary) {
      headers["Content-Type"] = "application/json";
    }
    return headers;
  }
  makeUrl(params) {
    const baseUrl = this.makeBaseUrl(params);
    if (params.model.includes(":")) {
      return `${baseUrl}/v1/predictions`;
    }
    return `${baseUrl}/v1/models/${params.model}/predictions`;
  }
};
var ReplicateTextToImageTask = class extends ReplicateTask {
  async getResponse(res, url, headers, outputType) {
    if (typeof res === "object" && "output" in res && Array.isArray(res.output) && res.output.length > 0 && typeof res.output[0] === "string") {
      if (outputType === "url") {
        return res.output[0];
      }
      const urlResponse = await fetch(res.output[0]);
      return await urlResponse.blob();
    }
    throw new InferenceOutputError("Expected Replicate text-to-image response format");
  }
};
var ReplicateTextToSpeechTask = class extends ReplicateTask {
  preparePayload(params) {
    const payload = super.preparePayload(params);
    const input = payload["input"];
    if (typeof input === "object" && input !== null && "prompt" in input) {
      const inputObj = input;
      inputObj["text"] = inputObj["prompt"];
      delete inputObj["prompt"];
    }
    return payload;
  }
  async getResponse(response) {
    if (response instanceof Blob) {
      return response;
    }
    if (response && typeof response === "object") {
      if ("output" in response) {
        if (typeof response.output === "string") {
          const urlResponse = await fetch(response.output);
          return await urlResponse.blob();
        } else if (Array.isArray(response.output)) {
          const urlResponse = await fetch(response.output[0]);
          return await urlResponse.blob();
        }
      }
    }
    throw new InferenceOutputError("Expected Blob or object with output");
  }
};
var ReplicateTextToVideoTask = class extends ReplicateTask {
  async getResponse(response) {
    if (typeof response === "object" && !!response && "output" in response && typeof response.output === "string" && isUrl(response.output)) {
      const urlResponse = await fetch(response.output);
      return await urlResponse.blob();
    }
    throw new InferenceOutputError("Expected { output: string }");
  }
};

// src/providers/sambanova.ts
var SambanovaConversationalTask = class extends BaseConversationalTask {
  constructor() {
    super("sambanova", "https://api.sambanova.ai");
  }
};

// src/providers/together.ts
var TOGETHER_API_BASE_URL = "https://api.together.xyz";
var TogetherConversationalTask = class extends BaseConversationalTask {
  constructor() {
    super("together", TOGETHER_API_BASE_URL);
  }
};
var TogetherTextGenerationTask = class extends BaseTextGenerationTask {
  constructor() {
    super("together", TOGETHER_API_BASE_URL);
  }
  preparePayload(params) {
    return {
      model: params.model,
      ...params.args,
      prompt: params.args.inputs
    };
  }
  async getResponse(response) {
    if (typeof response === "object" && "choices" in response && Array.isArray(response?.choices) && typeof response?.model === "string") {
      const completion = response.choices[0];
      return {
        generated_text: completion.text
      };
    }
    throw new InferenceOutputError("Expected Together text generation response format");
  }
};
var TogetherTextToImageTask = class extends TaskProviderHelper {
  constructor() {
    super("together", TOGETHER_API_BASE_URL);
  }
  makeRoute() {
    return "v1/images/generations";
  }
  preparePayload(params) {
    return {
      ...omit(params.args, ["inputs", "parameters"]),
      ...params.args.parameters,
      prompt: params.args.inputs,
      response_format: "base64",
      model: params.model
    };
  }
  async getResponse(response, outputType) {
    if (typeof response === "object" && "data" in response && Array.isArray(response.data) && response.data.length > 0 && "b64_json" in response.data[0] && typeof response.data[0].b64_json === "string") {
      const base64Data = response.data[0].b64_json;
      if (outputType === "url") {
        return `data:image/jpeg;base64,${base64Data}`;
      }
      return fetch(`data:image/jpeg;base64,${base64Data}`).then((res) => res.blob());
    }
    throw new InferenceOutputError("Expected Together text-to-image response format");
  }
};

// src/lib/getProviderHelper.ts
var PROVIDERS = {
  "black-forest-labs": {
    "text-to-image": new BlackForestLabsTextToImageTask()
  },
  cerebras: {
    conversational: new CerebrasConversationalTask()
  },
  cohere: {
    conversational: new CohereConversationalTask()
  },
  "fal-ai": {
    "text-to-image": new FalAITextToImageTask(),
    "text-to-speech": new FalAITextToSpeechTask(),
    "text-to-video": new FalAITextToVideoTask(),
    "automatic-speech-recognition": new FalAIAutomaticSpeechRecognitionTask()
  },
  "hf-inference": {
    "text-to-image": new HFInferenceTextToImageTask(),
    conversational: new HFInferenceConversationalTask(),
    "text-generation": new HFInferenceTextGenerationTask(),
    "text-classification": new HFInferenceTextClassificationTask(),
    "question-answering": new HFInferenceQuestionAnsweringTask(),
    "audio-classification": new HFInferenceAudioClassificationTask(),
    "automatic-speech-recognition": new HFInferenceAutomaticSpeechRecognitionTask(),
    "fill-mask": new HFInferenceFillMaskTask(),
    "feature-extraction": new HFInferenceFeatureExtractionTask(),
    "image-classification": new HFInferenceImageClassificationTask(),
    "image-segmentation": new HFInferenceImageSegmentationTask(),
    "document-question-answering": new HFInferenceDocumentQuestionAnsweringTask(),
    "image-to-text": new HFInferenceImageToTextTask(),
    "object-detection": new HFInferenceObjectDetectionTask(),
    "audio-to-audio": new HFInferenceAudioToAudioTask(),
    "zero-shot-image-classification": new HFInferenceZeroShotImageClassificationTask(),
    "zero-shot-classification": new HFInferenceZeroShotClassificationTask(),
    "image-to-image": new HFInferenceImageToImageTask(),
    "sentence-similarity": new HFInferenceSentenceSimilarityTask(),
    "table-question-answering": new HFInferenceTableQuestionAnsweringTask(),
    "tabular-classification": new HFInferenceTabularClassificationTask(),
    "text-to-speech": new HFInferenceTextToSpeechTask(),
    "token-classification": new HFInferenceTokenClassificationTask(),
    translation: new HFInferenceTranslationTask(),
    summarization: new HFInferenceSummarizationTask(),
    "visual-question-answering": new HFInferenceVisualQuestionAnsweringTask(),
    "tabular-regression": new HFInferenceTabularRegressionTask(),
    "text-to-audio": new HFInferenceTextToAudioTask()
  },
  "fireworks-ai": {
    conversational: new FireworksConversationalTask()
  },
  hyperbolic: {
    "text-to-image": new HyperbolicTextToImageTask(),
    conversational: new HyperbolicConversationalTask(),
    "text-generation": new HyperbolicTextGenerationTask()
  },
  nebius: {
    "text-to-image": new NebiusTextToImageTask(),
    conversational: new NebiusConversationalTask(),
    "text-generation": new NebiusTextGenerationTask()
  },
  novita: {
    conversational: new NovitaConversationalTask(),
    "text-generation": new NovitaTextGenerationTask()
  },
  openai: {
    conversational: new OpenAIConversationalTask()
  },
  replicate: {
    "text-to-image": new ReplicateTextToImageTask(),
    "text-to-speech": new ReplicateTextToSpeechTask(),
    "text-to-video": new ReplicateTextToVideoTask()
  },
  sambanova: {
    conversational: new SambanovaConversationalTask()
  },
  together: {
    "text-to-image": new TogetherTextToImageTask(),
    conversational: new TogetherConversationalTask(),
    "text-generation": new TogetherTextGenerationTask()
  }
};
function getProviderHelper(provider, task) {
  if (provider === "hf-inference") {
    if (!task) {
      return new HFInferenceTask();
    }
  }
  if (!task) {
    throw new Error("you need to provide a task name when using an external provider, e.g. 'text-to-image'");
  }
  if (!(provider in PROVIDERS)) {
    throw new Error(`Provider '${provider}' not supported. Available providers: ${Object.keys(PROVIDERS)}`);
  }
  const providerTasks = PROVIDERS[provider];
  if (!providerTasks || !(task in providerTasks)) {
    throw new Error(
      `Task '${task}' not supported for provider '${provider}'. Available tasks: ${Object.keys(providerTasks ?? {})}`
    );
  }
  return providerTasks[task];
}

// package.json
var name = "@huggingface/inference";
var version = "3.8.1";

// src/providers/consts.ts
var HARDCODED_MODEL_INFERENCE_MAPPING = {
  /**
   * "HF model ID" => "Model ID on Inference Provider's side"
   *
   * Example:
   * "Qwen/Qwen2.5-Coder-32B-Instruct": "Qwen2.5-Coder-32B-Instruct",
   */
  "black-forest-labs": {},
  cerebras: {},
  cohere: {},
  "fal-ai": {},
  "fireworks-ai": {},
  "hf-inference": {},
  hyperbolic: {},
  nebius: {},
  novita: {},
  openai: {},
  replicate: {},
  sambanova: {},
  together: {}
};

// src/lib/getInferenceProviderMapping.ts
var inferenceProviderMappingCache = /* @__PURE__ */ new Map();
async function getInferenceProviderMapping(params, options) {
  if (HARDCODED_MODEL_INFERENCE_MAPPING[params.provider][params.modelId]) {
    return HARDCODED_MODEL_INFERENCE_MAPPING[params.provider][params.modelId];
  }
  let inferenceProviderMapping;
  if (inferenceProviderMappingCache.has(params.modelId)) {
    inferenceProviderMapping = inferenceProviderMappingCache.get(params.modelId);
  } else {
    const resp = await (options?.fetch ?? fetch)(
      `${HF_HUB_URL}/api/models/${params.modelId}?expand[]=inferenceProviderMapping`,
      {
        headers: params.accessToken?.startsWith("hf_") ? { Authorization: `Bearer ${params.accessToken}` } : {}
      }
    );
    if (resp.status === 404) {
      throw new Error(`Model ${params.modelId} does not exist`);
    }
    inferenceProviderMapping = await resp.json().then((json) => json.inferenceProviderMapping).catch(() => null);
  }
  if (!inferenceProviderMapping) {
    throw new Error(`We have not been able to find inference provider information for model ${params.modelId}.`);
  }
  const providerMapping = inferenceProviderMapping[params.provider];
  if (providerMapping) {
    const equivalentTasks = params.provider === "hf-inference" && typedInclude(EQUIVALENT_SENTENCE_TRANSFORMERS_TASKS, params.task) ? EQUIVALENT_SENTENCE_TRANSFORMERS_TASKS : [params.task];
    if (!typedInclude(equivalentTasks, providerMapping.task)) {
      throw new Error(
        `Model ${params.modelId} is not supported for task ${params.task} and provider ${params.provider}. Supported task: ${providerMapping.task}.`
      );
    }
    if (providerMapping.status === "staging") {
      console.warn(
        `Model ${params.modelId} is in staging mode for provider ${params.provider}. Meant for test purposes only.`
      );
    }
    if (providerMapping.adapter === "lora") {
      const treeResp = await (options?.fetch ?? fetch)(`${HF_HUB_URL}/api/models/${params.modelId}/tree/main`);
      if (!treeResp.ok) {
        throw new Error(`Unable to fetch the model tree for ${params.modelId}.`);
      }
      const tree = await treeResp.json();
      const adapterWeightsPath = tree.find(({ type, path }) => type === "file" && path.endsWith(".safetensors"))?.path;
      if (!adapterWeightsPath) {
        throw new Error(`No .safetensors file found in the model tree for ${params.modelId}.`);
      }
      return {
        ...providerMapping,
        hfModelId: params.modelId,
        adapterWeightsPath
      };
    }
    return { ...providerMapping, hfModelId: params.modelId };
  }
  return null;
}

// src/lib/makeRequestOptions.ts
var tasks = null;
async function makeRequestOptions(args, providerHelper, options) {
  const { provider: maybeProvider, model: maybeModel } = args;
  const provider = maybeProvider ?? "hf-inference";
  const { task } = options ?? {};
  if (args.endpointUrl && provider !== "hf-inference") {
    throw new Error(`Cannot use endpointUrl with a third-party provider.`);
  }
  if (maybeModel && isUrl(maybeModel)) {
    throw new Error(`Model URLs are no longer supported. Use endpointUrl instead.`);
  }
  if (args.endpointUrl) {
    return makeRequestOptionsFromResolvedModel(
      maybeModel ?? args.endpointUrl,
      providerHelper,
      args,
      void 0,
      options
    );
  }
  if (!maybeModel && !task) {
    throw new Error("No model provided, and no task has been specified.");
  }
  const hfModel = maybeModel ?? await loadDefaultModel(task);
  if (providerHelper.clientSideRoutingOnly && !maybeModel) {
    throw new Error(`Provider ${provider} requires a model ID to be passed directly.`);
  }
  const inferenceProviderMapping = providerHelper.clientSideRoutingOnly ? {
    // eslint-disable-next-line @typescript-eslint/no-non-null-assertion
    providerId: removeProviderPrefix(maybeModel, provider),
    // eslint-disable-next-line @typescript-eslint/no-non-null-assertion
    hfModelId: maybeModel,
    status: "live",
    // eslint-disable-next-line @typescript-eslint/no-non-null-assertion
    task
  } : await getInferenceProviderMapping(
    {
      modelId: hfModel,
      // eslint-disable-next-line @typescript-eslint/no-non-null-assertion
      task,
      provider,
      accessToken: args.accessToken
    },
    { fetch: options?.fetch }
  );
  if (!inferenceProviderMapping) {
    throw new Error(`We have not been able to find inference provider information for model ${hfModel}.`);
  }
  return makeRequestOptionsFromResolvedModel(
    inferenceProviderMapping.providerId,
    providerHelper,
    args,
    inferenceProviderMapping,
    options
  );
}
function makeRequestOptionsFromResolvedModel(resolvedModel, providerHelper, args, mapping, options) {
  const { accessToken, endpointUrl, provider: maybeProvider, model, ...remainingArgs } = args;
  const provider = maybeProvider ?? "hf-inference";
  const { includeCredentials, task, signal, billTo } = options ?? {};
  const authMethod = (() => {
    if (providerHelper.clientSideRoutingOnly) {
      if (accessToken && accessToken.startsWith("hf_")) {
        throw new Error(`Provider ${provider} is closed-source and does not support HF tokens.`);
      }
      return "provider-key";
    }
    if (accessToken) {
      return accessToken.startsWith("hf_") ? "hf-token" : "provider-key";
    }
    if (includeCredentials === "include") {
      return "credentials-include";
    }
    return "none";
  })();
  const modelId = endpointUrl ?? resolvedModel;
  const url = providerHelper.makeUrl({
    authMethod,
    model: modelId,
    task
  });
  const headers = providerHelper.prepareHeaders(
    {
      accessToken,
      authMethod
    },
    "data" in args && !!args.data
  );
  if (billTo) {
    headers[HF_HEADER_X_BILL_TO] = billTo;
  }
  const ownUserAgent = `${name}/${version}`;
  const userAgent = [ownUserAgent, typeof navigator !== "undefined" ? navigator.userAgent : void 0].filter((x) => x !== void 0).join(" ");
  headers["User-Agent"] = userAgent;
  const body = providerHelper.makeBody({
    args: remainingArgs,
    model: resolvedModel,
    task,
    mapping
  });
  let credentials;
  if (typeof includeCredentials === "string") {
    credentials = includeCredentials;
  } else if (includeCredentials === true) {
    credentials = "include";
  }
  const info = {
    headers,
    method: "POST",
    body,
    ...credentials ? { credentials } : void 0,
    signal
  };
  return { url, info };
}
async function loadDefaultModel(task) {
  if (!tasks) {
    tasks = await loadTaskInfo();
  }
  const taskInfo = tasks[task];
  if ((taskInfo?.models.length ?? 0) <= 0) {
    throw new Error(`No default model defined for task ${task}, please define the model explicitly.`);
  }
  return taskInfo.models[0].id;
}
async function loadTaskInfo() {
  const res = await fetch(`${HF_HUB_URL}/api/tasks`);
  if (!res.ok) {
    throw new Error("Failed to load tasks definitions from Hugging Face Hub.");
  }
  return await res.json();
}
function removeProviderPrefix(model, provider) {
  if (!model.startsWith(`${provider}/`)) {
    throw new Error(`Models from ${provider} must be prefixed by "${provider}/". Got "${model}".`);
  }
  return model.slice(provider.length + 1);
}

// src/vendor/fetch-event-source/parse.ts
function getLines(onLine) {
  let buffer;
  let position;
  let fieldLength;
  let discardTrailingNewline = false;
  return function onChunk(arr) {
    if (buffer === void 0) {
      buffer = arr;
      position = 0;
      fieldLength = -1;
    } else {
      buffer = concat(buffer, arr);
    }
    const bufLength = buffer.length;
    let lineStart = 0;
    while (position < bufLength) {
      if (discardTrailingNewline) {
        if (buffer[position] === 10 /* NewLine */) {
          lineStart = ++position;
        }
        discardTrailingNewline = false;
      }
      let lineEnd = -1;
      for (; position < bufLength && lineEnd === -1; ++position) {
        switch (buffer[position]) {
          case 58 /* Colon */:
            if (fieldLength === -1) {
              fieldLength = position - lineStart;
            }
            break;
          case 13 /* CarriageReturn */:
            discardTrailingNewline = true;
          case 10 /* NewLine */:
            lineEnd = position;
            break;
        }
      }
      if (lineEnd === -1) {
        break;
      }
      onLine(buffer.subarray(lineStart, lineEnd), fieldLength);
      lineStart = position;
      fieldLength = -1;
    }
    if (lineStart === bufLength) {
      buffer = void 0;
    } else if (lineStart !== 0) {
      buffer = buffer.subarray(lineStart);
      position -= lineStart;
    }
  };
}
function getMessages(onId, onRetry, onMessage) {
  let message = newMessage();
  const decoder = new TextDecoder();
  return function onLine(line, fieldLength) {
    if (line.length === 0) {
      onMessage?.(message);
      message = newMessage();
    } else if (fieldLength > 0) {
      const field = decoder.decode(line.subarray(0, fieldLength));
      const valueOffset = fieldLength + (line[fieldLength + 1] === 32 /* Space */ ? 2 : 1);
      const value = decoder.decode(line.subarray(valueOffset));
      switch (field) {
        case "data":
          message.data = message.data ? message.data + "\n" + value : value;
          break;
        case "event":
          message.event = value;
          break;
        case "id":
          onId(message.id = value);
          break;
        case "retry":
          const retry = parseInt(value, 10);
          if (!isNaN(retry)) {
            onRetry(message.retry = retry);
          }
          break;
      }
    }
  };
}
function concat(a, b) {
  const res = new Uint8Array(a.length + b.length);
  res.set(a);
  res.set(b, a.length);
  return res;
}
function newMessage() {
  return {
    data: "",
    event: "",
    id: "",
    retry: void 0
  };
}

// src/utils/request.ts
async function innerRequest(args, providerHelper, options) {
  const { url, info } = await makeRequestOptions(args, providerHelper, options);
  const response = await (options?.fetch ?? fetch)(url, info);
  const requestContext = { url, info };
  if (options?.retry_on_error !== false && response.status === 503) {
    return innerRequest(args, providerHelper, options);
  }
  if (!response.ok) {
    const contentType = response.headers.get("Content-Type");
    if (["application/json", "application/problem+json"].some((ct) => contentType?.startsWith(ct))) {
      const output = await response.json();
      if ([400, 422, 404, 500].includes(response.status) && options?.chatCompletion) {
        throw new Error(
          `Server ${args.model} does not seem to support chat completion. Error: ${JSON.stringify(output.error)}`
        );
      }
      if (output.error || output.detail) {
        throw new Error(JSON.stringify(output.error ?? output.detail));
      } else {
        throw new Error(output);
      }
    }
    const message = contentType?.startsWith("text/plain;") ? await response.text() : void 0;
    throw new Error(message ?? "An error occurred while fetching the blob");
  }
  if (response.headers.get("Content-Type")?.startsWith("application/json")) {
    const data = await response.json();
    return { data, requestContext };
  }
  const blob = await response.blob();
  return { data: blob, requestContext };
}
async function* innerStreamingRequest(args, providerHelper, options) {
  const { url, info } = await makeRequestOptions({ ...args, stream: true }, providerHelper, options);
  const response = await (options?.fetch ?? fetch)(url, info);
  if (options?.retry_on_error !== false && response.status === 503) {
    return yield* innerStreamingRequest(args, providerHelper, options);
  }
  if (!response.ok) {
    if (response.headers.get("Content-Type")?.startsWith("application/json")) {
      const output = await response.json();
      if ([400, 422, 404, 500].includes(response.status) && options?.chatCompletion) {
        throw new Error(`Server ${args.model} does not seem to support chat completion. Error: ${output.error}`);
      }
      if (typeof output.error === "string") {
        throw new Error(output.error);
      }
      if (output.error && "message" in output.error && typeof output.error.message === "string") {
        throw new Error(output.error.message);
      }
      if (typeof output.message === "string") {
        throw new Error(output.message);
      }
    }
    throw new Error(`Server response contains error: ${response.status}`);
  }
  if (!response.headers.get("content-type")?.startsWith("text/event-stream")) {
    throw new Error(
      `Server does not support event stream content type, it returned ` + response.headers.get("content-type")
    );
  }
  if (!response.body) {
    return;
  }
  const reader = response.body.getReader();
  let events = [];
  const onEvent = (event) => {
    events.push(event);
  };
  const onChunk = getLines(
    getMessages(
      () => {
      },
      () => {
      },
      onEvent
    )
  );
  try {
    while (true) {
      const { done, value } = await reader.read();
      if (done) {
        return;
      }
      onChunk(value);
      for (const event of events) {
        if (event.data.length > 0) {
          if (event.data === "[DONE]") {
            return;
          }
          const data = JSON.parse(event.data);
          if (typeof data === "object" && data !== null && "error" in data) {
            const errorStr = typeof data.error === "string" ? data.error : typeof data.error === "object" && data.error && "message" in data.error && typeof data.error.message === "string" ? data.error.message : JSON.stringify(data.error);
            throw new Error(`Error forwarded from backend: ` + errorStr);
          }
          yield data;
        }
      }
      events = [];
    }
  } finally {
    reader.releaseLock();
  }
}

// src/tasks/custom/request.ts
async function request(args, options) {
  console.warn(
    "The request method is deprecated and will be removed in a future version of huggingface.js. Use specific task functions instead."
  );
  const providerHelper = getProviderHelper(args.provider ?? "hf-inference", options?.task);
  const result = await innerRequest(args, providerHelper, options);
  return result.data;
}

// src/tasks/custom/streamingRequest.ts
async function* streamingRequest(args, options) {
  console.warn(
    "The streamingRequest method is deprecated and will be removed in a future version of huggingface.js. Use specific task functions instead."
  );
  const providerHelper = getProviderHelper(args.provider ?? "hf-inference", options?.task);
  yield* innerStreamingRequest(args, providerHelper, options);
}

// src/tasks/audio/utils.ts
function preparePayload(args) {
  return "data" in args ? args : {
    ...omit(args, "inputs"),
    data: args.inputs
  };
}

// src/tasks/audio/audioClassification.ts
async function audioClassification(args, options) {
  const providerHelper = getProviderHelper(args.provider ?? "hf-inference", "audio-classification");
  const payload = preparePayload(args);
  const { data: res } = await innerRequest(payload, providerHelper, {
    ...options,
    task: "audio-classification"
  });
  return providerHelper.getResponse(res);
}

// src/tasks/audio/audioToAudio.ts
async function audioToAudio(args, options) {
  const providerHelper = getProviderHelper(args.provider ?? "hf-inference", "audio-to-audio");
  const payload = preparePayload(args);
  const { data: res } = await innerRequest(payload, providerHelper, {
    ...options,
    task: "audio-to-audio"
  });
  return providerHelper.getResponse(res);
}

// src/utils/base64FromBytes.ts
function base64FromBytes(arr) {
  if (globalThis.Buffer) {
    return globalThis.Buffer.from(arr).toString("base64");
  } else {
    const bin = [];
    arr.forEach((byte) => {
      bin.push(String.fromCharCode(byte));
    });
    return globalThis.btoa(bin.join(""));
  }
}

// src/tasks/audio/automaticSpeechRecognition.ts
async function automaticSpeechRecognition(args, options) {
  const providerHelper = getProviderHelper(args.provider ?? "hf-inference", "automatic-speech-recognition");
  const payload = await buildPayload(args);
  const { data: res } = await innerRequest(payload, providerHelper, {
    ...options,
    task: "automatic-speech-recognition"
  });
  const isValidOutput = typeof res?.text === "string";
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected {text: string}");
  }
  return providerHelper.getResponse(res);
}
async function buildPayload(args) {
  if (args.provider === "fal-ai") {
    const blob = "data" in args && args.data instanceof Blob ? args.data : "inputs" in args ? args.inputs : void 0;
    const contentType = blob?.type;
    if (!contentType) {
      throw new Error(
        `Unable to determine the input's content-type. Make sure your are passing a Blob when using provider fal-ai.`
      );
    }
    if (!FAL_AI_SUPPORTED_BLOB_TYPES.includes(contentType)) {
      throw new Error(
        `Provider fal-ai does not support blob type ${contentType} - supported content types are: ${FAL_AI_SUPPORTED_BLOB_TYPES.join(
          ", "
        )}`
      );
    }
    const base64audio = base64FromBytes(new Uint8Array(await blob.arrayBuffer()));
    return {
      ..."data" in args ? omit(args, "data") : omit(args, "inputs"),
      audio_url: `data:${contentType};base64,${base64audio}`
    };
  } else {
    return preparePayload(args);
  }
}

// src/tasks/audio/textToSpeech.ts
async function textToSpeech(args, options) {
  const provider = args.provider ?? "hf-inference";
  const providerHelper = getProviderHelper(provider, "text-to-speech");
  const { data: res } = await innerRequest(args, providerHelper, {
    ...options,
    task: "text-to-speech"
  });
  return providerHelper.getResponse(res);
}

// src/tasks/cv/utils.ts
function preparePayload2(args) {
  return "data" in args ? args : { ...omit(args, "inputs"), data: args.inputs };
}

// src/tasks/cv/imageClassification.ts
async function imageClassification(args, options) {
  const providerHelper = getProviderHelper(args.provider ?? "hf-inference", "image-classification");
  const payload = preparePayload2(args);
  const { data: res } = await innerRequest(payload, providerHelper, {
    ...options,
    task: "image-classification"
  });
  return providerHelper.getResponse(res);
}

// src/tasks/cv/imageSegmentation.ts
async function imageSegmentation(args, options) {
  const providerHelper = getProviderHelper(args.provider ?? "hf-inference", "image-segmentation");
  const payload = preparePayload2(args);
  const { data: res } = await innerRequest(payload, providerHelper, {
    ...options,
    task: "image-segmentation"
  });
  return providerHelper.getResponse(res);
}

// src/tasks/cv/imageToImage.ts
async function imageToImage(args, options) {
  const providerHelper = getProviderHelper(args.provider ?? "hf-inference", "image-to-image");
  let reqArgs;
  if (!args.parameters) {
    reqArgs = {
      accessToken: args.accessToken,
      model: args.model,
      data: args.inputs
    };
  } else {
    reqArgs = {
      ...args,
      inputs: base64FromBytes(
        new Uint8Array(args.inputs instanceof ArrayBuffer ? args.inputs : await args.inputs.arrayBuffer())
      )
    };
  }
  const { data: res } = await innerRequest(reqArgs, providerHelper, {
    ...options,
    task: "image-to-image"
  });
  return providerHelper.getResponse(res);
}

// src/tasks/cv/imageToText.ts
async function imageToText(args, options) {
  const providerHelper = getProviderHelper(args.provider ?? "hf-inference", "image-to-text");
  const payload = preparePayload2(args);
  const { data: res } = await innerRequest(payload, providerHelper, {
    ...options,
    task: "image-to-text"
  });
  return providerHelper.getResponse(res[0]);
}

// src/tasks/cv/objectDetection.ts
async function objectDetection(args, options) {
  const providerHelper = getProviderHelper(args.provider ?? "hf-inference", "object-detection");
  const payload = preparePayload2(args);
  const { data: res } = await innerRequest(payload, providerHelper, {
    ...options,
    task: "object-detection"
  });
  return providerHelper.getResponse(res);
}

// src/tasks/cv/textToImage.ts
async function textToImage(args, options) {
  const provider = args.provider ?? "hf-inference";
  const providerHelper = getProviderHelper(provider, "text-to-image");
  const { data: res } = await innerRequest(args, providerHelper, {
    ...options,
    task: "text-to-image"
  });
  const { url, info } = await makeRequestOptions(args, providerHelper, { ...options, task: "text-to-image" });
  return providerHelper.getResponse(res, url, info.headers, options?.outputType);
}

// src/tasks/cv/textToVideo.ts
async function textToVideo(args, options) {
  const provider = args.provider ?? "hf-inference";
  const providerHelper = getProviderHelper(provider, "text-to-video");
  const { data: response } = await innerRequest(
    args,
    providerHelper,
    {
      ...options,
      task: "text-to-video"
    }
  );
  const { url, info } = await makeRequestOptions(args, providerHelper, { ...options, task: "text-to-video" });
  return providerHelper.getResponse(response, url, info.headers);
}

// src/tasks/cv/zeroShotImageClassification.ts
async function preparePayload3(args) {
  if (args.inputs instanceof Blob) {
    return {
      ...args,
      inputs: {
        image: base64FromBytes(new Uint8Array(await args.inputs.arrayBuffer()))
      }
    };
  } else {
    return {
      ...args,
      inputs: {
        image: base64FromBytes(
          new Uint8Array(
            args.inputs.image instanceof ArrayBuffer ? args.inputs.image : await args.inputs.image.arrayBuffer()
          )
        )
      }
    };
  }
}
async function zeroShotImageClassification(args, options) {
  const providerHelper = getProviderHelper(args.provider ?? "hf-inference", "zero-shot-image-classification");
  const payload = await preparePayload3(args);
  const { data: res } = await innerRequest(payload, providerHelper, {
    ...options,
    task: "zero-shot-image-classification"
  });
  return providerHelper.getResponse(res);
}

// src/tasks/nlp/chatCompletion.ts
async function chatCompletion(args, options) {
  const providerHelper = getProviderHelper(args.provider ?? "hf-inference", "conversational");
  const { data: response } = await innerRequest(args, providerHelper, {
    ...options,
    task: "conversational"
  });
  return providerHelper.getResponse(response);
}

// src/tasks/nlp/chatCompletionStream.ts
async function* chatCompletionStream(args, options) {
  const providerHelper = getProviderHelper(args.provider ?? "hf-inference", "conversational");
  yield* innerStreamingRequest(args, providerHelper, {
    ...options,
    task: "conversational"
  });
}

// src/tasks/nlp/featureExtraction.ts
async function featureExtraction(args, options) {
  const providerHelper = getProviderHelper(args.provider ?? "hf-inference", "feature-extraction");
  const { data: res } = await innerRequest(args, providerHelper, {
    ...options,
    task: "feature-extraction"
  });
  return providerHelper.getResponse(res);
}

// src/tasks/nlp/fillMask.ts
async function fillMask(args, options) {
  const providerHelper = getProviderHelper(args.provider ?? "hf-inference", "fill-mask");
  const { data: res } = await innerRequest(args, providerHelper, {
    ...options,
    task: "fill-mask"
  });
  return providerHelper.getResponse(res);
}

// src/tasks/nlp/questionAnswering.ts
async function questionAnswering(args, options) {
  const providerHelper = getProviderHelper(args.provider ?? "hf-inference", "question-answering");
  const { data: res } = await innerRequest(
    args,
    providerHelper,
    {
      ...options,
      task: "question-answering"
    }
  );
  return providerHelper.getResponse(res);
}

// src/tasks/nlp/sentenceSimilarity.ts
async function sentenceSimilarity(args, options) {
  const providerHelper = getProviderHelper(args.provider ?? "hf-inference", "sentence-similarity");
  const { data: res } = await innerRequest(args, providerHelper, {
    ...options,
    task: "sentence-similarity"
  });
  return providerHelper.getResponse(res);
}

// src/tasks/nlp/summarization.ts
async function summarization(args, options) {
  const providerHelper = getProviderHelper(args.provider ?? "hf-inference", "summarization");
  const { data: res } = await innerRequest(args, providerHelper, {
    ...options,
    task: "summarization"
  });
  return providerHelper.getResponse(res);
}

// src/tasks/nlp/tableQuestionAnswering.ts
async function tableQuestionAnswering(args, options) {
  const providerHelper = getProviderHelper(args.provider ?? "hf-inference", "table-question-answering");
  const { data: res } = await innerRequest(
    args,
    providerHelper,
    {
      ...options,
      task: "table-question-answering"
    }
  );
  return providerHelper.getResponse(res);
}

// src/tasks/nlp/textClassification.ts
async function textClassification(args, options) {
  const providerHelper = getProviderHelper(args.provider ?? "hf-inference", "text-classification");
  const { data: res } = await innerRequest(args, providerHelper, {
    ...options,
    task: "text-classification"
  });
  return providerHelper.getResponse(res);
}

// src/tasks/nlp/textGeneration.ts
async function textGeneration(args, options) {
  const providerHelper = getProviderHelper(args.provider ?? "hf-inference", "text-generation");
  const { data: response } = await innerRequest(args, providerHelper, {
    ...options,
    task: "text-generation"
  });
  return providerHelper.getResponse(response);
}

// src/tasks/nlp/textGenerationStream.ts
async function* textGenerationStream(args, options) {
  const providerHelper = getProviderHelper(args.provider ?? "hf-inference", "text-generation");
  yield* innerStreamingRequest(args, providerHelper, {
    ...options,
    task: "text-generation"
  });
}

// src/tasks/nlp/tokenClassification.ts
async function tokenClassification(args, options) {
  const providerHelper = getProviderHelper(args.provider ?? "hf-inference", "token-classification");
  const { data: res } = await innerRequest(
    args,
    providerHelper,
    {
      ...options,
      task: "token-classification"
    }
  );
  return providerHelper.getResponse(res);
}

// src/tasks/nlp/translation.ts
async function translation(args, options) {
  const providerHelper = getProviderHelper(args.provider ?? "hf-inference", "translation");
  const { data: res } = await innerRequest(args, providerHelper, {
    ...options,
    task: "translation"
  });
  return providerHelper.getResponse(res);
}

// src/tasks/nlp/zeroShotClassification.ts
async function zeroShotClassification(args, options) {
  const providerHelper = getProviderHelper(args.provider ?? "hf-inference", "zero-shot-classification");
  const { data: res } = await innerRequest(
    args,
    providerHelper,
    {
      ...options,
      task: "zero-shot-classification"
    }
  );
  return providerHelper.getResponse(res);
}

// src/tasks/multimodal/documentQuestionAnswering.ts
async function documentQuestionAnswering(args, options) {
  const providerHelper = getProviderHelper(args.provider ?? "hf-inference", "document-question-answering");
  const reqArgs = {
    ...args,
    inputs: {
      question: args.inputs.question,
      // convert Blob or ArrayBuffer to base64
      image: base64FromBytes(new Uint8Array(await args.inputs.image.arrayBuffer()))
    }
  };
  const { data: res } = await innerRequest(
    reqArgs,
    providerHelper,
    {
      ...options,
      task: "document-question-answering"
    }
  );
  return providerHelper.getResponse(res);
}

// src/tasks/multimodal/visualQuestionAnswering.ts
async function visualQuestionAnswering(args, options) {
  const providerHelper = getProviderHelper(args.provider ?? "hf-inference", "visual-question-answering");
  const reqArgs = {
    ...args,
    inputs: {
      question: args.inputs.question,
      // convert Blob or ArrayBuffer to base64
      image: base64FromBytes(new Uint8Array(await args.inputs.image.arrayBuffer()))
    }
  };
  const { data: res } = await innerRequest(reqArgs, providerHelper, {
    ...options,
    task: "visual-question-answering"
  });
  return providerHelper.getResponse(res);
}

// src/tasks/tabular/tabularClassification.ts
async function tabularClassification(args, options) {
  const providerHelper = getProviderHelper(args.provider ?? "hf-inference", "tabular-classification");
  const { data: res } = await innerRequest(args, providerHelper, {
    ...options,
    task: "tabular-classification"
  });
  return providerHelper.getResponse(res);
}

// src/tasks/tabular/tabularRegression.ts
async function tabularRegression(args, options) {
  const providerHelper = getProviderHelper(args.provider ?? "hf-inference", "tabular-regression");
  const { data: res } = await innerRequest(args, providerHelper, {
    ...options,
    task: "tabular-regression"
  });
  return providerHelper.getResponse(res);
}

// src/InferenceClient.ts
var InferenceClient = class {
  accessToken;
  defaultOptions;
  constructor(accessToken = "", defaultOptions = {}) {
    this.accessToken = accessToken;
    this.defaultOptions = defaultOptions;
    for (const [name2, fn] of Object.entries(tasks_exports)) {
      Object.defineProperty(this, name2, {
        enumerable: false,
        value: (params, options) => (
          // eslint-disable-next-line @typescript-eslint/no-explicit-any
          fn({ ...params, accessToken }, { ...defaultOptions, ...options })
        )
      });
    }
  }
  /**
   * Returns copy of InferenceClient tied to a specified endpoint.
   */
  endpoint(endpointUrl) {
    return new InferenceClientEndpoint(endpointUrl, this.accessToken, this.defaultOptions);
  }
};
var InferenceClientEndpoint = class {
  constructor(endpointUrl, accessToken = "", defaultOptions = {}) {
    accessToken;
    defaultOptions;
    for (const [name2, fn] of Object.entries(tasks_exports)) {
      Object.defineProperty(this, name2, {
        enumerable: false,
        value: (params, options) => (
          // eslint-disable-next-line @typescript-eslint/no-explicit-any
          fn({ ...params, accessToken, endpointUrl }, { ...defaultOptions, ...options })
        )
      });
    }
  }
};
var HfInference = class extends InferenceClient {
};

// src/types.ts
var INFERENCE_PROVIDERS = [
  "black-forest-labs",
  "cerebras",
  "cohere",
  "fal-ai",
  "fireworks-ai",
  "hf-inference",
  "hyperbolic",
  "nebius",
  "novita",
  "openai",
  "replicate",
  "sambanova",
  "together"
];

// src/snippets/index.ts
var snippets_exports = {};
__export(snippets_exports, {
  getInferenceSnippets: () => getInferenceSnippets
});

// src/snippets/getInferenceSnippets.ts
var import_jinja = require("@huggingface/jinja");
var import_tasks = require("@huggingface/tasks");

// src/snippets/templates.exported.ts
var templates = {
  "js": {
    "fetch": {
      "basic": 'async function query(data) {\n	const response = await fetch(\n		"{{ fullUrl }}",\n		{\n			headers: {\n				Authorization: "{{ authorizationHeader }}",\n				"Content-Type": "application/json",\n{% if billTo %}\n				"X-HF-Bill-To": "{{ billTo }}",\n{% endif %}			},\n			method: "POST",\n			body: JSON.stringify(data),\n		}\n	);\n	const result = await response.json();\n	return result;\n}\n\nquery({ inputs: {{ providerInputs.asObj.inputs }} }).then((response) => {\n    console.log(JSON.stringify(response));\n});',
      "basicAudio": 'async function query(data) {\n	const response = await fetch(\n		"{{ fullUrl }}",\n		{\n			headers: {\n				Authorization: "{{ authorizationHeader }}",\n				"Content-Type": "audio/flac",\n{% if billTo %}\n				"X-HF-Bill-To": "{{ billTo }}",\n{% endif %}			},\n			method: "POST",\n			body: JSON.stringify(data),\n		}\n	);\n	const result = await response.json();\n	return result;\n}\n\nquery({ inputs: {{ providerInputs.asObj.inputs }} }).then((response) => {\n    console.log(JSON.stringify(response));\n});',
      "basicImage": 'async function query(data) {\n	const response = await fetch(\n		"{{ fullUrl }}",\n		{\n			headers: {\n				Authorization: "{{ authorizationHeader }}",\n				"Content-Type": "image/jpeg",\n{% if billTo %}\n				"X-HF-Bill-To": "{{ billTo }}",\n{% endif %}			},\n			method: "POST",\n			body: JSON.stringify(data),\n		}\n	);\n	const result = await response.json();\n	return result;\n}\n\nquery({ inputs: {{ providerInputs.asObj.inputs }} }).then((response) => {\n    console.log(JSON.stringify(response));\n});',
      "textToAudio": '{% if model.library_name == "transformers" %}\nasync function query(data) {\n	const response = await fetch(\n		"{{ fullUrl }}",\n		{\n			headers: {\n				Authorization: "{{ authorizationHeader }}",\n				"Content-Type": "application/json",\n{% if billTo %}\n				"X-HF-Bill-To": "{{ billTo }}",\n{% endif %}			},\n			method: "POST",\n			body: JSON.stringify(data),\n		}\n	);\n	const result = await response.blob();\n    return result;\n}\n\nquery({ inputs: {{ providerInputs.asObj.inputs }} }).then((response) => {\n    // Returns a byte object of the Audio wavform. Use it directly!\n});\n{% else %}\nasync function query(data) {\n	const response = await fetch(\n		"{{ fullUrl }}",\n		{\n			headers: {\n				Authorization: "{{ authorizationHeader }}",\n				"Content-Type": "application/json",\n			},\n			method: "POST",\n			body: JSON.stringify(data),\n		}\n	);\n    const result = await response.json();\n    return result;\n}\n\nquery({ inputs: {{ providerInputs.asObj.inputs }} }).then((response) => {\n    console.log(JSON.stringify(response));\n});\n{% endif %} ',
      "textToImage": 'async function query(data) {\n	const response = await fetch(\n		"{{ fullUrl }}",\n		{\n			headers: {\n				Authorization: "{{ authorizationHeader }}",\n				"Content-Type": "application/json",\n{% if billTo %}\n				"X-HF-Bill-To": "{{ billTo }}",\n{% endif %}			},\n			method: "POST",\n			body: JSON.stringify(data),\n		}\n	);\n	const result = await response.blob();\n	return result;\n}\n\n\nquery({ {{ providerInputs.asTsString }} }).then((response) => {\n    // Use image\n});',
      "zeroShotClassification": 'async function query(data) {\n    const response = await fetch(\n		"{{ fullUrl }}",\n        {\n            headers: {\n				Authorization: "{{ authorizationHeader }}",\n                "Content-Type": "application/json",\n{% if billTo %}\n                "X-HF-Bill-To": "{{ billTo }}",\n{% endif %}         },\n            method: "POST",\n            body: JSON.stringify(data),\n        }\n    );\n    const result = await response.json();\n    return result;\n}\n\nquery({\n    inputs: {{ providerInputs.asObj.inputs }},\n    parameters: { candidate_labels: ["refund", "legal", "faq"] }\n}).then((response) => {\n    console.log(JSON.stringify(response));\n});'
    },
    "huggingface.js": {
      "basic": 'import { InferenceClient } from "@huggingface/inference";\n\nconst client = new InferenceClient("{{ accessToken }}");\n\nconst output = await client.{{ methodName }}({\n	model: "{{ model.id }}",\n	inputs: {{ inputs.asObj.inputs }},\n	provider: "{{ provider }}",\n}{% if billTo %}, {\n	billTo: "{{ billTo }}",\n}{% endif %});\n\nconsole.log(output);',
      "basicAudio": 'import { InferenceClient } from "@huggingface/inference";\n\nconst client = new InferenceClient("{{ accessToken }}");\n\nconst data = fs.readFileSync({{inputs.asObj.inputs}});\n\nconst output = await client.{{ methodName }}({\n	data,\n	model: "{{ model.id }}",\n	provider: "{{ provider }}",\n}{% if billTo %}, {\n	billTo: "{{ billTo }}",\n}{% endif %});\n\nconsole.log(output);',
      "basicImage": 'import { InferenceClient } from "@huggingface/inference";\n\nconst client = new InferenceClient("{{ accessToken }}");\n\nconst data = fs.readFileSync({{inputs.asObj.inputs}});\n\nconst output = await client.{{ methodName }}({\n	data,\n	model: "{{ model.id }}",\n	provider: "{{ provider }}",\n}{% if billTo %}, {\n	billTo: "{{ billTo }}",\n}{% endif %});\n\nconsole.log(output);',
      "conversational": 'import { InferenceClient } from "@huggingface/inference";\n\nconst client = new InferenceClient("{{ accessToken }}");\n\nconst chatCompletion = await client.chatCompletion({\n    provider: "{{ provider }}",\n    model: "{{ model.id }}",\n{{ inputs.asTsString }}\n}{% if billTo %}, {\n    billTo: "{{ billTo }}",\n}{% endif %});\n\nconsole.log(chatCompletion.choices[0].message);',
      "conversationalStream": 'import { InferenceClient } from "@huggingface/inference";\n\nconst client = new InferenceClient("{{ accessToken }}");\n\nlet out = "";\n\nconst stream = await client.chatCompletionStream({\n    provider: "{{ provider }}",\n    model: "{{ model.id }}",\n{{ inputs.asTsString }}\n}{% if billTo %}, {\n    billTo: "{{ billTo }}",\n}{% endif %});\n\nfor await (const chunk of stream) {\n	if (chunk.choices && chunk.choices.length > 0) {\n		const newContent = chunk.choices[0].delta.content;\n		out += newContent;\n		console.log(newContent);\n	}  \n}',
      "textToImage": `import { InferenceClient } from "@huggingface/inference";

const client = new InferenceClient("{{ accessToken }}");

const image = await client.textToImage({
    provider: "{{ provider }}",
    model: "{{ model.id }}",
	inputs: {{ inputs.asObj.inputs }},
	parameters: { num_inference_steps: 5 },
}{% if billTo %}, {
    billTo: "{{ billTo }}",
}{% endif %});
/// Use the generated image (it's a Blob)`,
      "textToVideo": `import { InferenceClient } from "@huggingface/inference";

const client = new InferenceClient("{{ accessToken }}");

const image = await client.textToVideo({
    provider: "{{ provider }}",
    model: "{{ model.id }}",
	inputs: {{ inputs.asObj.inputs }},
}{% if billTo %}, {
    billTo: "{{ billTo }}",
}{% endif %});
// Use the generated video (it's a Blob)`
    },
    "openai": {
      "conversational": 'import { OpenAI } from "openai";\n\nconst client = new OpenAI({\n	baseURL: "{{ baseUrl }}",\n	apiKey: "{{ accessToken }}",\n{% if billTo %}\n	defaultHeaders: {\n		"X-HF-Bill-To": "{{ billTo }}" \n	}\n{% endif %}\n});\n\nconst chatCompletion = await client.chat.completions.create({\n	model: "{{ providerModelId }}",\n{{ inputs.asTsString }}\n});\n\nconsole.log(chatCompletion.choices[0].message);',
      "conversationalStream": 'import { OpenAI } from "openai";\n\nconst client = new OpenAI({\n	baseURL: "{{ baseUrl }}",\n	apiKey: "{{ accessToken }}",\n{% if billTo %}\n    defaultHeaders: {\n		"X-HF-Bill-To": "{{ billTo }}" \n	}\n{% endif %}\n});\n\nconst stream = await client.chat.completions.create({\n    model: "{{ providerModelId }}",\n{{ inputs.asTsString }}\n    stream: true,\n});\n\nfor await (const chunk of stream) {\n    process.stdout.write(chunk.choices[0]?.delta?.content || "");\n}'
    }
  },
  "python": {
    "fal_client": {
      "textToImage": '{% if provider == "fal-ai" %}\nimport fal_client\n\nresult = fal_client.subscribe(\n    "{{ providerModelId }}",\n    arguments={\n        "prompt": {{ inputs.asObj.inputs }},\n    },\n)\nprint(result)\n{% endif %} '
    },
    "huggingface_hub": {
      "basic": 'result = client.{{ methodName }}(\n    inputs={{ inputs.asObj.inputs }},\n    model="{{ model.id }}",\n)',
      "basicAudio": 'output = client.{{ methodName }}({{ inputs.asObj.inputs }}, model="{{ model.id }}")',
      "basicImage": 'output = client.{{ methodName }}({{ inputs.asObj.inputs }}, model="{{ model.id }}")',
      "conversational": 'completion = client.chat.completions.create(\n    model="{{ model.id }}",\n{{ inputs.asPythonString }}\n)\n\nprint(completion.choices[0].message) ',
      "conversationalStream": 'stream = client.chat.completions.create(\n    model="{{ model.id }}",\n{{ inputs.asPythonString }}\n    stream=True,\n)\n\nfor chunk in stream:\n    print(chunk.choices[0].delta.content, end="") ',
      "documentQuestionAnswering": 'output = client.document_question_answering(\n    "{{ inputs.asObj.image }}",\n    question="{{ inputs.asObj.question }}",\n    model="{{ model.id }}",\n) ',
      "imageToImage": '# output is a PIL.Image object\nimage = client.image_to_image(\n    "{{ inputs.asObj.inputs }}",\n    prompt="{{ inputs.asObj.parameters.prompt }}",\n    model="{{ model.id }}",\n) ',
      "importInferenceClient": 'from huggingface_hub import InferenceClient\n\nclient = InferenceClient(\n    provider="{{ provider }}",\n    api_key="{{ accessToken }}",\n{% if billTo %}\n    bill_to="{{ billTo }}",\n{% endif %}\n)',
      "textToImage": '# output is a PIL.Image object\nimage = client.text_to_image(\n    {{ inputs.asObj.inputs }},\n    model="{{ model.id }}",\n) ',
      "textToVideo": 'video = client.text_to_video(\n    {{ inputs.asObj.inputs }},\n    model="{{ model.id }}",\n) '
    },
    "openai": {
      "conversational": 'from openai import OpenAI\n\nclient = OpenAI(\n    base_url="{{ baseUrl }}",\n    api_key="{{ accessToken }}",\n{% if billTo %}\n    default_headers={\n        "X-HF-Bill-To": "{{ billTo }}"\n    }\n{% endif %}\n)\n\ncompletion = client.chat.completions.create(\n    model="{{ providerModelId }}",\n{{ inputs.asPythonString }}\n)\n\nprint(completion.choices[0].message) ',
      "conversationalStream": 'from openai import OpenAI\n\nclient = OpenAI(\n    base_url="{{ baseUrl }}",\n    api_key="{{ accessToken }}",\n{% if billTo %}\n    default_headers={\n        "X-HF-Bill-To": "{{ billTo }}"\n    }\n{% endif %}\n)\n\nstream = client.chat.completions.create(\n    model="{{ providerModelId }}",\n{{ inputs.asPythonString }}\n    stream=True,\n)\n\nfor chunk in stream:\n    print(chunk.choices[0].delta.content, end="")'
    },
    "requests": {
      "basic": 'def query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.json()\n\noutput = query({\n    "inputs": {{ providerInputs.asObj.inputs }},\n}) ',
      "basicAudio": 'def query(filename):\n    with open(filename, "rb") as f:\n        data = f.read()\n    response = requests.post(API_URL, headers={"Content-Type": "audio/flac", **headers}, data=data)\n    return response.json()\n\noutput = query({{ providerInputs.asObj.inputs }})',
      "basicImage": 'def query(filename):\n    with open(filename, "rb") as f:\n        data = f.read()\n    response = requests.post(API_URL, headers={"Content-Type": "image/jpeg", **headers}, data=data)\n    return response.json()\n\noutput = query({{ providerInputs.asObj.inputs }})',
      "conversational": 'def query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.json()\n\nresponse = query({\n{{ providerInputs.asJsonString }}\n})\n\nprint(response["choices"][0]["message"])',
      "conversationalStream": 'def query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload, stream=True)\n    for line in response.iter_lines():\n        if not line.startswith(b"data:"):\n            continue\n        if line.strip() == b"data: [DONE]":\n            return\n        yield json.loads(line.decode("utf-8").lstrip("data:").rstrip("/n"))\n\nchunks = query({\n{{ providerInputs.asJsonString }},\n    "stream": True,\n})\n\nfor chunk in chunks:\n    print(chunk["choices"][0]["delta"]["content"], end="")',
      "documentQuestionAnswering": 'def query(payload):\n    with open(payload["image"], "rb") as f:\n        img = f.read()\n        payload["image"] = base64.b64encode(img).decode("utf-8")\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.json()\n\noutput = query({\n    "inputs": {\n        "image": "{{ inputs.asObj.image }}",\n        "question": "{{ inputs.asObj.question }}",\n    },\n}) ',
      "imageToImage": 'def query(payload):\n    with open(payload["inputs"], "rb") as f:\n        img = f.read()\n        payload["inputs"] = base64.b64encode(img).decode("utf-8")\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.content\n\nimage_bytes = query({\n{{ providerInputs.asJsonString }}\n})\n\n# You can access the image with PIL.Image for example\nimport io\nfrom PIL import Image\nimage = Image.open(io.BytesIO(image_bytes)) ',
      "importRequests": '{% if importBase64 %}\nimport base64\n{% endif %}\n{% if importJson %}\nimport json\n{% endif %}\nimport requests\n\nAPI_URL = "{{ fullUrl }}"\nheaders = {\n    "Authorization": "{{ authorizationHeader }}",\n{% if billTo %}\n    "X-HF-Bill-To": "{{ billTo }}"\n{% endif %}\n}',
      "tabular": 'def query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.content\n\nresponse = query({\n    "inputs": {\n        "data": {{ providerInputs.asObj.inputs }}\n    },\n}) ',
      "textToAudio": '{% if model.library_name == "transformers" %}\ndef query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.content\n\naudio_bytes = query({\n    "inputs": {{ providerInputs.asObj.inputs }},\n})\n# You can access the audio with IPython.display for example\nfrom IPython.display import Audio\nAudio(audio_bytes)\n{% else %}\ndef query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.json()\n\naudio, sampling_rate = query({\n    "inputs": {{ providerInputs.asObj.inputs }},\n})\n# You can access the audio with IPython.display for example\nfrom IPython.display import Audio\nAudio(audio, rate=sampling_rate)\n{% endif %} ',
      "textToImage": '{% if provider == "hf-inference" %}\ndef query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.content\n\nimage_bytes = query({\n    "inputs": {{ providerInputs.asObj.inputs }},\n})\n\n# You can access the image with PIL.Image for example\nimport io\nfrom PIL import Image\nimage = Image.open(io.BytesIO(image_bytes))\n{% endif %}',
      "zeroShotClassification": 'def query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.json()\n\noutput = query({\n    "inputs": {{ providerInputs.asObj.inputs }},\n    "parameters": {"candidate_labels": ["refund", "legal", "faq"]},\n}) ',
      "zeroShotImageClassification": 'def query(data):\n    with open(data["image_path"], "rb") as f:\n        img = f.read()\n    payload={\n        "parameters": data["parameters"],\n        "inputs": base64.b64encode(img).decode("utf-8")\n    }\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.json()\n\noutput = query({\n    "image_path": {{ providerInputs.asObj.inputs }},\n    "parameters": {"candidate_labels": ["cat", "dog", "llama"]},\n}) '
    }
  },
  "sh": {
    "curl": {
      "basic": "curl {{ fullUrl }} \\\n    -X POST \\\n    -H 'Authorization: {{ authorizationHeader }}' \\\n    -H 'Content-Type: application/json' \\\n{% if billTo %}\n    -H 'X-HF-Bill-To: {{ billTo }}' \\\n{% endif %}\n    -d '{\n{{ providerInputs.asCurlString }}\n    }'",
      "basicAudio": "curl {{ fullUrl }} \\\n    -X POST \\\n    -H 'Authorization: {{ authorizationHeader }}' \\\n    -H 'Content-Type: audio/flac' \\\n{% if billTo %}\n    -H 'X-HF-Bill-To: {{ billTo }}' \\\n{% endif %}\n    --data-binary @{{ providerInputs.asObj.inputs }}",
      "basicImage": "curl {{ fullUrl }} \\\n    -X POST \\\n    -H 'Authorization: {{ authorizationHeader }}' \\\n    -H 'Content-Type: image/jpeg' \\\n{% if billTo %}\n    -H 'X-HF-Bill-To: {{ billTo }}' \\\n{% endif %}\n    --data-binary @{{ providerInputs.asObj.inputs }}",
      "conversational": `curl {{ fullUrl }} \\
    -H 'Authorization: {{ authorizationHeader }}' \\
    -H 'Content-Type: application/json' \\
{% if billTo %}
    -H 'X-HF-Bill-To: {{ billTo }}' \\
{% endif %}
    -d '{
{{ providerInputs.asCurlString }},
        "stream": false
    }'`,
      "conversationalStream": `curl {{ fullUrl }} \\
    -H 'Authorization: {{ authorizationHeader }}' \\
    -H 'Content-Type: application/json' \\
{% if billTo %}
    -H 'X-HF-Bill-To: {{ billTo }}' \\
{% endif %}
    -d '{
{{ providerInputs.asCurlString }},
        "stream": true
    }'`,
      "zeroShotClassification": `curl {{ fullUrl }} \\
    -X POST \\
    -d '{"inputs": {{ providerInputs.asObj.inputs }}, "parameters": {"candidate_labels": ["refund", "legal", "faq"]}}' \\
    -H 'Content-Type: application/json' \\
    -H 'Authorization: {{ authorizationHeader }}'
{% if billTo %} \\
    -H 'X-HF-Bill-To: {{ billTo }}'
{% endif %}`
    }
  }
};

// src/snippets/getInferenceSnippets.ts
var PYTHON_CLIENTS = ["huggingface_hub", "fal_client", "requests", "openai"];
var JS_CLIENTS = ["fetch", "huggingface.js", "openai"];
var SH_CLIENTS = ["curl"];
var CLIENTS = {
  js: [...JS_CLIENTS],
  python: [...PYTHON_CLIENTS],
  sh: [...SH_CLIENTS]
};
var hasTemplate = (language, client, templateName) => templates[language]?.[client]?.[templateName] !== void 0;
var loadTemplate = (language, client, templateName) => {
  const template = templates[language]?.[client]?.[templateName];
  if (!template) {
    throw new Error(`Template not found: ${language}/${client}/${templateName}`);
  }
  return (data) => new import_jinja.Template(template).render({ ...data });
};
var snippetImportPythonInferenceClient = loadTemplate("python", "huggingface_hub", "importInferenceClient");
var snippetImportRequests = loadTemplate("python", "requests", "importRequests");
var HF_PYTHON_METHODS = {
  "audio-classification": "audio_classification",
  "audio-to-audio": "audio_to_audio",
  "automatic-speech-recognition": "automatic_speech_recognition",
  "document-question-answering": "document_question_answering",
  "feature-extraction": "feature_extraction",
  "fill-mask": "fill_mask",
  "image-classification": "image_classification",
  "image-segmentation": "image_segmentation",
  "image-to-image": "image_to_image",
  "image-to-text": "image_to_text",
  "object-detection": "object_detection",
  "question-answering": "question_answering",
  "sentence-similarity": "sentence_similarity",
  summarization: "summarization",
  "table-question-answering": "table_question_answering",
  "tabular-classification": "tabular_classification",
  "tabular-regression": "tabular_regression",
  "text-classification": "text_classification",
  "text-generation": "text_generation",
  "text-to-image": "text_to_image",
  "text-to-speech": "text_to_speech",
  "text-to-video": "text_to_video",
  "token-classification": "token_classification",
  translation: "translation",
  "visual-question-answering": "visual_question_answering",
  "zero-shot-classification": "zero_shot_classification",
  "zero-shot-image-classification": "zero_shot_image_classification"
};
var HF_JS_METHODS = {
  "automatic-speech-recognition": "automaticSpeechRecognition",
  "feature-extraction": "featureExtraction",
  "fill-mask": "fillMask",
  "image-classification": "imageClassification",
  "question-answering": "questionAnswering",
  "sentence-similarity": "sentenceSimilarity",
  summarization: "summarization",
  "table-question-answering": "tableQuestionAnswering",
  "text-classification": "textClassification",
  "text-generation": "textGeneration",
  "text2text-generation": "textGeneration",
  "token-classification": "tokenClassification",
  translation: "translation"
};
var snippetGenerator = (templateName, inputPreparationFn) => {
  return (model, accessToken, provider, inferenceProviderMapping, opts) => {
    const providerModelId = inferenceProviderMapping?.providerId ?? model.id;
    let task = model.pipeline_tag;
    if (model.pipeline_tag && ["text-generation", "image-text-to-text"].includes(model.pipeline_tag) && model.tags.includes("conversational")) {
      templateName = opts?.streaming ? "conversationalStream" : "conversational";
      inputPreparationFn = prepareConversationalInput;
      task = "conversational";
    }
    let providerHelper;
    try {
      providerHelper = getProviderHelper(provider, task);
    } catch (e) {
      console.error(`Failed to get provider helper for ${provider} (${task})`, e);
      return [];
    }
    const inputs = inputPreparationFn ? inputPreparationFn(model, opts) : { inputs: (0, import_tasks.getModelInputSnippet)(model) };
    const request2 = makeRequestOptionsFromResolvedModel(
      providerModelId,
      providerHelper,
      {
        accessToken,
        provider,
        ...inputs
      },
      inferenceProviderMapping,
      {
        task,
        billTo: opts?.billTo
      }
    );
    let providerInputs = inputs;
    const bodyAsObj = request2.info.body;
    if (typeof bodyAsObj === "string") {
      try {
        providerInputs = JSON.parse(bodyAsObj);
      } catch (e) {
        console.error("Failed to parse body as JSON", e);
      }
    }
    const params = {
      accessToken,
      authorizationHeader: request2.info.headers?.Authorization,
      baseUrl: removeSuffix(request2.url, "/chat/completions"),
      fullUrl: request2.url,
      inputs: {
        asObj: inputs,
        asCurlString: formatBody(inputs, "curl"),
        asJsonString: formatBody(inputs, "json"),
        asPythonString: formatBody(inputs, "python"),
        asTsString: formatBody(inputs, "ts")
      },
      providerInputs: {
        asObj: providerInputs,
        asCurlString: formatBody(providerInputs, "curl"),
        asJsonString: formatBody(providerInputs, "json"),
        asPythonString: formatBody(providerInputs, "python"),
        asTsString: formatBody(providerInputs, "ts")
      },
      model,
      provider,
      providerModelId: providerModelId ?? model.id,
      billTo: opts?.billTo
    };
    return import_tasks.inferenceSnippetLanguages.map((language) => {
      return CLIENTS[language].map((client) => {
        if (!hasTemplate(language, client, templateName)) {
          return;
        }
        const template = loadTemplate(language, client, templateName);
        if (client === "huggingface_hub" && templateName.includes("basic")) {
          if (!(model.pipeline_tag && model.pipeline_tag in HF_PYTHON_METHODS)) {
            return;
          }
          params["methodName"] = HF_PYTHON_METHODS[model.pipeline_tag];
        }
        if (client === "huggingface.js" && templateName.includes("basic")) {
          if (!(model.pipeline_tag && model.pipeline_tag in HF_JS_METHODS)) {
            return;
          }
          params["methodName"] = HF_JS_METHODS[model.pipeline_tag];
        }
        let snippet = template(params).trim();
        if (!snippet) {
          return;
        }
        if (client === "huggingface_hub") {
          const importSection = snippetImportPythonInferenceClient({ ...params });
          snippet = `${importSection}

${snippet}`;
        } else if (client === "requests") {
          const importSection = snippetImportRequests({
            ...params,
            importBase64: snippet.includes("base64"),
            importJson: snippet.includes("json.")
          });
          snippet = `${importSection}

${snippet}`;
        }
        return { language, client, content: snippet };
      }).filter((snippet) => snippet !== void 0);
    }).flat();
  };
};
var prepareDocumentQuestionAnsweringInput = (model) => {
  return JSON.parse((0, import_tasks.getModelInputSnippet)(model));
};
var prepareImageToImageInput = (model) => {
  const data = JSON.parse((0, import_tasks.getModelInputSnippet)(model));
  return { inputs: data.image, parameters: { prompt: data.prompt } };
};
var prepareConversationalInput = (model, opts) => {
  return {
    messages: opts?.messages ?? (0, import_tasks.getModelInputSnippet)(model),
    ...opts?.temperature ? { temperature: opts?.temperature } : void 0,
    max_tokens: opts?.max_tokens ?? 512,
    ...opts?.top_p ? { top_p: opts?.top_p } : void 0
  };
};
var snippets = {
  "audio-classification": snippetGenerator("basicAudio"),
  "audio-to-audio": snippetGenerator("basicAudio"),
  "automatic-speech-recognition": snippetGenerator("basicAudio"),
  "document-question-answering": snippetGenerator("documentQuestionAnswering", prepareDocumentQuestionAnsweringInput),
  "feature-extraction": snippetGenerator("basic"),
  "fill-mask": snippetGenerator("basic"),
  "image-classification": snippetGenerator("basicImage"),
  "image-segmentation": snippetGenerator("basicImage"),
  "image-text-to-text": snippetGenerator("conversational"),
  "image-to-image": snippetGenerator("imageToImage", prepareImageToImageInput),
  "image-to-text": snippetGenerator("basicImage"),
  "object-detection": snippetGenerator("basicImage"),
  "question-answering": snippetGenerator("basic"),
  "sentence-similarity": snippetGenerator("basic"),
  summarization: snippetGenerator("basic"),
  "tabular-classification": snippetGenerator("tabular"),
  "tabular-regression": snippetGenerator("tabular"),
  "table-question-answering": snippetGenerator("basic"),
  "text-classification": snippetGenerator("basic"),
  "text-generation": snippetGenerator("basic"),
  "text-to-audio": snippetGenerator("textToAudio"),
  "text-to-image": snippetGenerator("textToImage"),
  "text-to-speech": snippetGenerator("textToAudio"),
  "text-to-video": snippetGenerator("textToVideo"),
  "text2text-generation": snippetGenerator("basic"),
  "token-classification": snippetGenerator("basic"),
  translation: snippetGenerator("basic"),
  "zero-shot-classification": snippetGenerator("zeroShotClassification"),
  "zero-shot-image-classification": snippetGenerator("zeroShotImageClassification")
};
function getInferenceSnippets(model, accessToken, provider, inferenceProviderMapping, opts) {
  return model.pipeline_tag && model.pipeline_tag in snippets ? snippets[model.pipeline_tag]?.(model, accessToken, provider, inferenceProviderMapping, opts) ?? [] : [];
}
function formatBody(obj, format) {
  switch (format) {
    case "curl":
      return indentString(formatBody(obj, "json"));
    case "json":
      return JSON.stringify(obj, null, 4).split("\n").slice(1, -1).join("\n");
    case "python":
      return indentString(
        Object.entries(obj).map(([key, value]) => {
          const formattedValue = JSON.stringify(value, null, 4).replace(/"/g, '"');
          return `${key}=${formattedValue},`;
        }).join("\n")
      );
    case "ts":
      return formatTsObject(obj).split("\n").slice(1, -1).join("\n");
    default:
      throw new Error(`Unsupported format: ${format}`);
  }
}
function formatTsObject(obj, depth) {
  depth = depth ?? 0;
  if (typeof obj !== "object" || obj === null) {
    return JSON.stringify(obj);
  }
  if (Array.isArray(obj)) {
    const items = obj.map((item) => {
      const formatted = formatTsObject(item, depth + 1);
      return `${" ".repeat(4 * (depth + 1))}${formatted},`;
    }).join("\n");
    return `[
${items}
${" ".repeat(4 * depth)}]`;
  }
  const entries = Object.entries(obj);
  const lines = entries.map(([key, value]) => {
    const formattedValue = formatTsObject(value, depth + 1);
    const keyStr = /^[a-zA-Z_$][a-zA-Z0-9_$]*$/.test(key) ? key : `"${key}"`;
    return `${" ".repeat(4 * (depth + 1))}${keyStr}: ${formattedValue},`;
  }).join("\n");
  return `{
${lines}
${" ".repeat(4 * depth)}}`;
}
function indentString(str) {
  return str.split("\n").map((line) => " ".repeat(4) + line).join("\n");
}
function removeSuffix(str, suffix) {
  return str.endsWith(suffix) ? str.slice(0, -suffix.length) : str;
}
// Annotate the CommonJS export names for ESM import in node:
0 && (module.exports = {
  HfInference,
  INFERENCE_PROVIDERS,
  InferenceClient,
  InferenceClientEndpoint,
  InferenceOutputError,
  audioClassification,
  audioToAudio,
  automaticSpeechRecognition,
  chatCompletion,
  chatCompletionStream,
  documentQuestionAnswering,
  featureExtraction,
  fillMask,
  imageClassification,
  imageSegmentation,
  imageToImage,
  imageToText,
  objectDetection,
  questionAnswering,
  request,
  sentenceSimilarity,
  snippets,
  streamingRequest,
  summarization,
  tableQuestionAnswering,
  tabularClassification,
  tabularRegression,
  textClassification,
  textGeneration,
  textGenerationStream,
  textToImage,
  textToSpeech,
  textToVideo,
  tokenClassification,
  translation,
  visualQuestionAnswering,
  zeroShotClassification,
  zeroShotImageClassification
});
